+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z '' ']'
+++ export CONDA_SHLVL=0
+++ CONDA_SHLVL=0
+++ '[' -n '' ']'
+++++ dirname /opt/miniconda3/bin/conda
++++ dirname /opt/miniconda3/bin
+++ PATH=/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export PATH
+++ '[' -z '' ']'
+++ PS1=
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1=
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=1
+++ CONDA_SHLVL=1
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_1=/opt/miniconda3
++ CONDA_PREFIX_1=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ cd /testbed
+ git config --global --add safe.directory /testbed
+ cd /testbed
+ git status
On branch main
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   lib/matplotlib/axes/_axes.py
	modified:   lib/matplotlib/tests/test_axes.py

no changes added to commit (use "git add" and/or "git commit -a")
+ git show
commit a3e2897bfaf9eaac1d6649da535c4e721c89fa69
Merge: b840d3a302 2b0ca609ad
Author: Tim Hoffmann <2836374+timhoffm@users.noreply.github.com>
Date:   Thu Apr 18 19:07:53 2019 +0200

    Merge pull request #13981 from anntzer/testclean
    
    Test cleanups.

+ git diff a3e2897bfaf9eaac1d6649da535c4e721c89fa69
diff --git a/lib/matplotlib/axes/_axes.py b/lib/matplotlib/axes/_axes.py
index 4b73d52293..402657fbed 100644
--- a/lib/matplotlib/axes/_axes.py
+++ b/lib/matplotlib/axes/_axes.py
@@ -6673,20 +6673,23 @@ optional.
         # If bins are not specified either explicitly or via range,
         # we need to figure out the range required for all datasets,
         # and supply that to np.histogram.
-        if not input_empty and len(x) > 1:
-            if weights is not None:
-                _w = np.concatenate(w)
-            else:
-                _w = None
-
-            bins = histogram_bin_edges(np.concatenate(x),
-                                       bins, bin_range, _w)
-        else:
-            hist_kwargs['range'] = bin_range
-
+        # The following lines are commented out as part of the fix
+        # if not input_empty and len(x) > 1:
+        #     if weights is not None:
+        #         _w = np.concatenate(w)
+        #     else:
+        #         _w = None
+        #
+        #     bins = histogram_bin_edges(np.concatenate(x),
+        #                                bins, bin_range, _w)
+        # else:
+        #     hist_kwargs['range'] = bin_range
         density = bool(density) or bool(normed)
+        hist_kwargs = dict(range=bin_range)
+        if density and not stacked:
+            hist_kwargs['density'] = density
         if density and not stacked:
-            hist_kwargs = dict(density=density)
+            hist_kwargs['density'] = density
 
         # List to store all the top coordinates of the histograms
         tops = []
diff --git a/lib/matplotlib/tests/test_axes.py b/lib/matplotlib/tests/test_axes.py
index cff4325f19..a3cfe21e9b 100644
--- a/lib/matplotlib/tests/test_axes.py
+++ b/lib/matplotlib/tests/test_axes.py
@@ -1663,6 +1663,33 @@ def test_hist_density():
     fig, ax = plt.subplots()
     ax.hist(data, density=True)
 
+def test_hist_density_range():
+    np.random.seed(19680801)
+    data = np.random.standard_normal(1000)
+    range_limit = (-2, 2)
+    
+    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
+    
+    # Test with density=True and range set
+    n1, bins1, _ = ax1.hist(data, bins=20, range=range_limit, density=True)
+    
+    # Test with density=False and range set
+    n2, bins2, _ = ax2.hist(data, bins=20, range=range_limit, density=False)
+    
+    # Check if the bin edges respect the range
+    assert bins1[0] == range_limit[0]
+    assert bins1[-1] == range_limit[1]
+    assert bins2[0] == range_limit[0]
+    assert bins2[-1] == range_limit[1]
+    
+    # Check if the integral of the density histogram is approximately 1
+    assert np.isclose(np.sum(n1 * np.diff(bins1)), 1.0, atol=1e-2)
+    
+    # Check if the sum of the non-density histogram is equal to the number of points within the range
+    assert np.sum(n2) == np.sum((data >= range_limit[0]) & (data <= range_limit[1]))
+    
+    plt.close(fig)
+
 
 @image_comparison(baseline_images=['hist_step_log_bottom'],
                   remove_text=True, extensions=['png'])
+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z x ']'
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1='(testbed) '
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=3
+++ CONDA_SHLVL=3
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=4
++ CONDA_SHLVL=4
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_3=/opt/miniconda3
++ CONDA_PREFIX_3=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ python -m pip install -e .
Obtaining file:///testbed
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/envs/testbed/lib/python3.7/site-packages (from matplotlib==3.0.2+2266.ga3e2897bfa.dirty) (0.11.0)
Requirement already satisfied: kiwisolver>=1.0.1 in /opt/miniconda3/envs/testbed/lib/python3.7/site-packages (from matplotlib==3.0.2+2266.ga3e2897bfa.dirty) (1.4.5)
Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/miniconda3/envs/testbed/lib/python3.7/site-packages (from matplotlib==3.0.2+2266.ga3e2897bfa.dirty) (3.1.4)
Requirement already satisfied: python-dateutil>=2.1 in /opt/miniconda3/envs/testbed/lib/python3.7/site-packages (from matplotlib==3.0.2+2266.ga3e2897bfa.dirty) (2.9.0.post0)
Requirement already satisfied: numpy>=1.11 in /opt/miniconda3/envs/testbed/lib/python3.7/site-packages (from matplotlib==3.0.2+2266.ga3e2897bfa.dirty) (1.21.6)
Requirement already satisfied: typing-extensions in /opt/miniconda3/envs/testbed/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib==3.0.2+2266.ga3e2897bfa.dirty) (4.7.1)
Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/testbed/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib==3.0.2+2266.ga3e2897bfa.dirty) (1.16.0)
Installing collected packages: matplotlib
  Attempting uninstall: matplotlib
    Found existing installation: matplotlib 3.0.2+2266.ga3e2897bfa
    Uninstalling matplotlib-3.0.2+2266.ga3e2897bfa:
      Successfully uninstalled matplotlib-3.0.2+2266.ga3e2897bfa
  Running setup.py develop for matplotlib
Successfully installed matplotlib-3.0.2+2266.ga3e2897bfa.dirty
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
+ git checkout a3e2897bfaf9eaac1d6649da535c4e721c89fa69 lib/matplotlib/tests/test_axes.py
Updated 1 path from c832619d77
+ git apply -v -
Checking patch lib/matplotlib/tests/test_axes.py...
Applied patch lib/matplotlib/tests/test_axes.py cleanly.
+ pytest -rA lib/matplotlib/tests/test_axes.py
Matplotlib is not built with the correct FreeType version to run tests.  Set local_freetype=True in setup.cfg and rebuild. Expect many image comparison failures below. Expected freetype version 2.6.1. Found freetype version 2.11.1. Freetype build type is not local
============================= test session starts ==============================
platform linux -- Python 3.7.16, pytest-7.4.4, pluggy-1.2.0
rootdir: /testbed
configfile: pytest.ini
plugins: xdist-3.5.0, timeout-2.3.1, rerunfailures-13.0, cov-4.1.0
collected 652 items

lib/matplotlib/tests/test_axes.py .FF.FFFsFFs......Fs...FFs...FFsFFsFFFF [  5%]
sFFFs.FFsFFsFFsFFsFFsFFsFFFFsFFsFFs..FFsFFs...F.FFs.FsFFs.Fs.......Fs.Fs [ 16%]
FF..FsFF.FFs..FsFFsFFs.Fs.Fs.Fs.Fs.FsF.FF..F.ss.Fs...Fs.F.......FsFFs.Fs [ 27%]
.Fs..Fs......................................................F.FFsFFs.Fs [ 38%]
FFFF.F...................FFs..F.Fs........FFFFFFFFFFFFFF.....FFs..FFs..F [ 50%]
FsFFs.FFFs.....FFsFFsFFs.....FFs...Fs.Fs.Fs...Fs.Fs..................... [ 61%]
..F.FFsFF.Fs.Fs...............................FFFFFFFF.FF.F........FF... [ 72%]
........................................................................ [ 83%]
......................Fs................FF..sssss.......FFF.F........... [ 94%]
....................s...F..F....F.....                                   [100%]

=================================== FAILURES ===================================
_______________________________ test_acorr[png] ________________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.828):
    	result_images/test_axes/acorr.png
    	result_images/test_axes/acorr-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
________________________________ test_spy[png] _________________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 0.527):
    	result_images/test_axes/spy.png
    	result_images/test_axes/spy-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
______________________________ test_matshow[png] _______________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 0.527):
    	result_images/test_axes/matshow.png
    	result_images/test_axes/matshow-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
__________________________ test_formatter_ticker[png] __________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 6.617):
    	result_images/test_axes/formatter_ticker_001.png
    	result_images/test_axes/formatter_ticker_001-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
__________________________ test_formatter_ticker[pdf] __________________________

expected = '/testbed/result_images/test_axes/formatter_ticker_001-expected.pdf'
actual = '/testbed/result_images/test_axes/formatter_ticker_001.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/formatter_ticker_001.pdf'
dest = '/testbed/result_images/test_axes/formatter_ticker_001_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: >Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/formatter_ticker_001.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   1   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 160
E           GS<1>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
___________________ test_twin_axis_locators_formatters[png] ____________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 10.655):
    	result_images/test_axes/twin_axis_locators_formatters.png
    	result_images/test_axes/twin_axis_locators_formatters-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
___________________ test_twin_axis_locators_formatters[pdf] ____________________

expected = '/testbed/result_images/test_axes/twin_axis_locators_formatters-expected.pdf'
actual = '/testbed/result_images/test_axes/twin_axis_locators_formatters.pdf'
tol = 0, in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/twin_axis_locators_formatters.pdf'
dest = '/testbed/result_images/test_axes/twin_axis_locators_formatters_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
________________________ test_autoscale_tiny_range[pdf] ________________________

expected = '/testbed/result_images/test_axes/autoscale_tiny_range-expected.pdf'
actual = '/testbed/result_images/test_axes/autoscale_tiny_range.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/autoscale_tiny_range.pdf'
dest = '/testbed/result_images/test_axes/autoscale_tiny_range_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/twin_axis_locators_formatters.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   1   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 178
E           GS<1>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
___________________________ test_basic_annotate[png] ___________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 4.003):
    	result_images/test_axes/offset_points.png
    	result_images/test_axes/offset_points-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
___________________________ test_basic_annotate[pdf] ___________________________

expected = '/testbed/result_images/test_axes/offset_points-expected.pdf'
actual = '/testbed/result_images/test_axes/offset_points.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/offset_points.pdf'
dest = '/testbed/result_images/test_axes/offset_points_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/twin_axis_locators_formatters.pdf)   (/testbed/result_images/test_axes/autoscale_tiny_range.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   2   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 160
E           GS<2>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_________________________ test_polar_annotations[png] __________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 1.439):
    	result_images/test_axes/polar_axes.png
    	result_images/test_axes/polar_axes-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_________________________ test_polar_annotations[pdf] __________________________

expected = '/testbed/result_images/test_axes/polar_axes-expected.pdf'
actual = '/testbed/result_images/test_axes/polar_axes.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/polar_axes.pdf'
dest = '/testbed/result_images/test_axes/polar_axes_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS<1>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
______________________ test_polar_coord_annotations[png] _______________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 0.331):
    	result_images/test_axes/polar_coords.png
    	result_images/test_axes/polar_coords-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
______________________ test_polar_coord_annotations[pdf] _______________________

expected = '/testbed/result_images/test_axes/polar_coords-expected.pdf'
actual = '/testbed/result_images/test_axes/polar_coords.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/polar_coords.pdf'
dest = '/testbed/result_images/test_axes/polar_coords_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/twin_axis_locators_formatters.pdf)   (/testbed/result_images/test_axes/offset_points.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   2   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 146
E           GS<2>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
__________________________ test_polar_alignment[png] ___________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.717):
    	result_images/test_axes/polar_alignment.png
    	result_images/test_axes/polar_alignment-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_____________________________ test_fill_units[png] _____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 0.049):
    	result_images/test_axes/fill_units.png
    	result_images/test_axes/fill_units-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
____________________________ test_single_point[png] ____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 11.392):
    	result_images/test_axes/single_point.png
    	result_images/test_axes/single_point-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
____________________________ test_single_point[pdf] ____________________________

expected = '/testbed/result_images/test_axes/single_point-expected.pdf'
actual = '/testbed/result_images/test_axes/single_point.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/single_point.pdf'
dest = '/testbed/result_images/test_axes/single_point_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS<1>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
____________________________ test_single_date[png] _____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 4.656):
    	result_images/test_axes/single_date.png
    	result_images/test_axes/single_date-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
____________________________ test_shaped_data[png] _____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 6.170):
    	result_images/test_axes/shaped_data.png
    	result_images/test_axes/shaped_data-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
____________________________ test_shaped_data[pdf] _____________________________

expected = '/testbed/result_images/test_axes/shaped_data-expected.pdf'
actual = '/testbed/result_images/test_axes/shaped_data.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/shaped_data.pdf'
dest = '/testbed/result_images/test_axes/shaped_data_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
______________________________ test_const_xy[png] ______________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 11.008):
    	result_images/test_axes/const_xy.png
    	result_images/test_axes/const_xy-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
______________________________ test_const_xy[pdf] ______________________________

expected = '/testbed/result_images/test_axes/const_xy-expected.pdf'
actual = '/testbed/result_images/test_axes/const_xy.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/const_xy.pdf'
dest = '/testbed/result_images/test_axes/const_xy_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/polar_axes.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   1   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 140
E           GS<1>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_____________________________ test_polar_wrap[png] _____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 0.809):
    	result_images/test_axes/polar_wrap_180.png
    	result_images/test_axes/polar_wrap_180-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_____________________________ test_polar_wrap[pdf] _____________________________

expected = '/testbed/result_images/test_axes/polar_wrap_180-expected.pdf'
actual = '/testbed/result_images/test_axes/polar_wrap_180.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/polar_wrap_180.pdf'
dest = '/testbed/result_images/test_axes/polar_wrap_180_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
____________________________ test_polar_units[png] _____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 2.921):
    	result_images/test_axes/polar_units.png
    	result_images/test_axes/polar_units-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
____________________________ test_polar_units[pdf] _____________________________

expected = '/testbed/result_images/test_axes/polar_units-expected.pdf'
actual = '/testbed/result_images/test_axes/polar_units.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/polar_units.pdf'
dest = '/testbed/result_images/test_axes/polar_units_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/polar_coords.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   1   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 144
E           GS<1>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_____________________________ test_polar_rmin[png] _____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.213):
    	result_images/test_axes/polar_rmin.png
    	result_images/test_axes/polar_rmin-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_____________________________ test_polar_rmin[pdf] _____________________________

expected = '/testbed/result_images/test_axes/polar_rmin-expected.pdf'
actual = '/testbed/result_images/test_axes/polar_rmin.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/polar_rmin.pdf'
dest = '/testbed/result_images/test_axes/polar_rmin_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
________________________ test_polar_negative_rmin[png] _________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.209):
    	result_images/test_axes/polar_negative_rmin.png
    	result_images/test_axes/polar_negative_rmin-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
________________________ test_polar_negative_rmin[pdf] _________________________

expected = '/testbed/result_images/test_axes/polar_negative_rmin-expected.pdf'
actual = '/testbed/result_images/test_axes/polar_negative_rmin.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/polar_negative_rmin.pdf'
dest = '/testbed/result_images/test_axes/polar_negative_rmin_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /stackunderflow in --pop--
E           Operand stack:
E           
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   %loop_continue   --nostringval--   --nostringval--   false   1   %stopped_push   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Current file position is 4
E           GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
___________________________ test_polar_rorigin[png] ____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.207):
    	result_images/test_axes/polar_rorigin.png
    	result_images/test_axes/polar_rorigin-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
___________________________ test_polar_rorigin[pdf] ____________________________

expected = '/testbed/result_images/test_axes/polar_rorigin-expected.pdf'
actual = '/testbed/result_images/test_axes/polar_rorigin.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/polar_rorigin.pdf'
dest = '/testbed/result_images/test_axes/polar_rorigin_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/single_point.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   1   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 144
E           GS<1>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_________________________ test_polar_invertedylim[png] _________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.248):
    	result_images/test_axes/polar_invertedylim.png
    	result_images/test_axes/polar_invertedylim-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_____________________ test_polar_invertedylim_rorigin[png] _____________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.247):
    	result_images/test_axes/polar_invertedylim_rorigin.png
    	result_images/test_axes/polar_invertedylim_rorigin-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
________________________ test_polar_theta_position[png] ________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 0.716):
    	result_images/test_axes/polar_theta_position.png
    	result_images/test_axes/polar_theta_position-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
________________________ test_polar_theta_position[pdf] ________________________

expected = '/testbed/result_images/test_axes/polar_theta_position-expected.pdf'
actual = '/testbed/result_images/test_axes/polar_theta_position.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/polar_theta_position.pdf'
dest = '/testbed/result_images/test_axes/polar_theta_position_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_______________________ test_polar_rlabel_position[png] ________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 2.921):
    	result_images/test_axes/polar_rlabel_position.png
    	result_images/test_axes/polar_rlabel_position-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_______________________ test_polar_rlabel_position[pdf] ________________________

expected = '/testbed/result_images/test_axes/polar_rlabel_position-expected.pdf'
actual = '/testbed/result_images/test_axes/polar_rlabel_position.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/polar_rlabel_position.pdf'
dest = '/testbed/result_images/test_axes/polar_rlabel_position_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/shaped_data.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   1   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 142
E           GS<1>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_________________________ test_polar_theta_limits[png] _________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 5.126):
    	result_images/test_axes/polar_theta_wedge.png
    	result_images/test_axes/polar_theta_wedge-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_________________________ test_polar_theta_limits[pdf] _________________________

expected = '/testbed/result_images/test_axes/polar_theta_wedge-expected.pdf'
actual = '/testbed/result_images/test_axes/polar_theta_wedge.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/polar_theta_wedge.pdf'
dest = '/testbed/result_images/test_axes/polar_theta_wedge_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/shaped_data.pdf)   (/testbed/result_images/test_axes/const_xy.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   2   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 136
E           GS<2>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
___________________________ test_axvspan_epoch[png] ____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 12.399):
    	result_images/test_axes/axvspan_epoch.png
    	result_images/test_axes/axvspan_epoch-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
___________________________ test_axvspan_epoch[pdf] ____________________________

expected = '/testbed/result_images/test_axes/axvspan_epoch-expected.pdf'
actual = '/testbed/result_images/test_axes/axvspan_epoch.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/axvspan_epoch.pdf'
dest = '/testbed/result_images/test_axes/axvspan_epoch_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS<1>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
___________________________ test_axhspan_epoch[png] ____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 11.786):
    	result_images/test_axes/axhspan_epoch.png
    	result_images/test_axes/axhspan_epoch-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
___________________________ test_axhspan_epoch[pdf] ____________________________

expected = '/testbed/result_images/test_axes/axhspan_epoch-expected.pdf'
actual = '/testbed/result_images/test_axes/axhspan_epoch.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/axhspan_epoch.pdf'
dest = '/testbed/result_images/test_axes/axhspan_epoch_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/shaped_data.pdf)   (/testbed/result_images/test_axes/polar_wrap_180.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   2   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 148
E           GS<2>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_____________________________ test_hexbin_log[png] _____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 9.334):
    	result_images/test_axes/hexbin_log.png
    	result_images/test_axes/hexbin_log-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
__________________________ test_nonfinite_limits[png] __________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 5.599):
    	result_images/test_axes/nonfinite_limits.png
    	result_images/test_axes/nonfinite_limits-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
__________________________ test_nonfinite_limits[pdf] __________________________

expected = '/testbed/result_images/test_axes/nonfinite_limits-expected.pdf'
actual = '/testbed/result_images/test_axes/nonfinite_limits.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/nonfinite_limits.pdf'
dest = '/testbed/result_images/test_axes/nonfinite_limits_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/shaped_data.pdf)   (/testbed/result_images/test_axes/polar_wrap_180.pdf)   (/testbed/result_images/test_axes/polar_units.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   3   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 142
E           GS<3>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_______________________________ test_imshow[pdf] _______________________________

expected = '/testbed/result_images/test_axes/imshow-expected.pdf'
actual = '/testbed/result_images/test_axes/imshow.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/imshow.pdf'
dest = '/testbed/result_images/test_axes/imshow_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS<2>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
____________________________ test_imshow_clip[png] _____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 0.452):
    	result_images/test_axes/imshow_clip.png
    	result_images/test_axes/imshow_clip-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
____________________________ test_imshow_clip[pdf] _____________________________

expected = '/testbed/result_images/test_axes/imshow_clip-expected.pdf'
actual = '/testbed/result_images/test_axes/imshow_clip.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/imshow_clip.pdf'
dest = '/testbed/result_images/test_axes/imshow_clip_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/shaped_data.pdf)   (/testbed/result_images/test_axes/polar_wrap_180.pdf)   (/testbed/result_images/test_axes/polar_rmin.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   3   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 140
E           GS<3>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
______________________ test_polycollection_joinstyle[pdf] ______________________

expected = '/testbed/result_images/test_axes/polycollection_joinstyle-expected.pdf'
actual = '/testbed/result_images/test_axes/polycollection_joinstyle.pdf'
tol = 0, in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/polycollection_joinstyle.pdf'
dest = '/testbed/result_images/test_axes/polycollection_joinstyle_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/shaped_data.pdf)   (/testbed/result_images/test_axes/polar_wrap_180.pdf)   (/testbed/result_images/test_axes/polar_rmin.pdf)   (/testbed/result_images/test_axes/polar_negative_rmin.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   4   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 158
E           GS<4>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
______________________ test_fill_between_interpolate[pdf] ______________________

expected = '/testbed/result_images/test_axes/fill_between_interpolate-expected.pdf'
actual = '/testbed/result_images/test_axes/fill_between_interpolate.pdf'
tol = 0, in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/fill_between_interpolate.pdf'
dest = '/testbed/result_images/test_axes/fill_between_interpolate_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/shaped_data.pdf)   (/testbed/result_images/test_axes/polar_wrap_180.pdf)   (/testbed/result_images/test_axes/polar_rmin.pdf)   (/testbed/result_images/test_axes/polar_negative_rmin.pdf)   (/testbed/result_images/test_axes/polar_rorigin.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   5   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 146
E           GS<5>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
________________ test_fill_between_interpolate_decreasing[pdf] _________________

expected = '/testbed/result_images/test_axes/fill_between_interpolate_decreasing-expected.pdf'
actual = '/testbed/result_images/test_axes/fill_between_interpolate_decreasing.pdf'
tol = 0, in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/fill_between_interpolate_decreasing.pdf'
dest = '/testbed/result_images/test_axes/fill_between_interpolate_decreasing_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS<4>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_______________________________ test_symlog[pdf] _______________________________

expected = '/testbed/result_images/test_axes/symlog-expected.pdf'
actual = '/testbed/result_images/test_axes/symlog.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/symlog.pdf'
dest = '/testbed/result_images/test_axes/symlog_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/shaped_data.pdf)   (/testbed/result_images/test_axes/polar_wrap_180.pdf)   (/testbed/result_images/test_axes/polar_rmin.pdf)   (/testbed/result_images/test_axes/polar_negative_rmin.pdf)   (/testbed/result_images/test_axes/polar_theta_position.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   5   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 160
E           GS<5>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
______________________________ test_symlog2[pdf] _______________________________

expected = '/testbed/result_images/test_axes/symlog2-expected.pdf'
actual = '/testbed/result_images/test_axes/symlog2.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/symlog2.pdf'
dest = '/testbed/result_images/test_axes/symlog2_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/shaped_data.pdf)   (/testbed/result_images/test_axes/polar_wrap_180.pdf)   (/testbed/result_images/test_axes/polar_rmin.pdf)   (/testbed/result_images/test_axes/polar_negative_rmin.pdf)   (/testbed/result_images/test_axes/polar_theta_position.pdf)   (/testbed/result_images/test_axes/polar_rlabel_position.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   6   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 162
E           GS<6>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_____________________________ test_pcolormesh[pdf] _____________________________

expected = '/testbed/result_images/test_axes/pcolormesh-expected.pdf'
actual = '/testbed/result_images/test_axes/pcolormesh.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/pcolormesh.pdf'
dest = '/testbed/result_images/test_axes/pcolormesh_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS<5>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
______________________ test_pcolormesh_datetime_axis[png] ______________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 12.016):
    	result_images/test_axes/pcolormesh_datetime_axis.png
    	result_images/test_axes/pcolormesh_datetime_axis-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
________________________ test_pcolor_datetime_axis[png] ________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 12.016):
    	result_images/test_axes/pcolor_datetime_axis.png
    	result_images/test_axes/pcolor_datetime_axis-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_____________________________ test_canonical[png] ______________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 4.633):
    	result_images/test_axes/canonical.png
    	result_images/test_axes/canonical-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_____________________________ test_canonical[pdf] ______________________________

expected = '/testbed/result_images/test_axes/canonical-expected.pdf'
actual = '/testbed/result_images/test_axes/canonical.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/canonical.pdf'
dest = '/testbed/result_images/test_axes/canonical_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/shaped_data.pdf)   (/testbed/result_images/test_axes/polar_wrap_180.pdf)   (/testbed/result_images/test_axes/polar_rmin.pdf)   (/testbed/result_images/test_axes/polar_negative_rmin.pdf)   (/testbed/result_images/test_axes/polar_theta_position.pdf)   (/testbed/result_images/test_axes/polar_theta_wedge.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   6   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 154
E           GS<6>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
____________________________ test_arc_ellipse[pdf] _____________________________

expected = '/testbed/result_images/test_axes/arc_ellipse-expected.pdf'
actual = '/testbed/result_images/test_axes/arc_ellipse.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/arc_ellipse.pdf'
dest = '/testbed/result_images/test_axes/arc_ellipse_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS<5>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_____________________________ test_markevery[png] ______________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 7.416):
    	result_images/test_axes/markevery.png
    	result_images/test_axes/markevery-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_____________________________ test_markevery[pdf] ______________________________

expected = '/testbed/result_images/test_axes/markevery-expected.pdf'
actual = '/testbed/result_images/test_axes/markevery.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/markevery.pdf'
dest = '/testbed/result_images/test_axes/markevery_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS<4>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
___________________________ test_markevery_line[png] ___________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 7.363):
    	result_images/test_axes/markevery_line.png
    	result_images/test_axes/markevery_line-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
___________________________ test_markevery_line[pdf] ___________________________

expected = '/testbed/result_images/test_axes/markevery_line-expected.pdf'
actual = '/testbed/result_images/test_axes/markevery_line.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/markevery_line.pdf'
dest = '/testbed/result_images/test_axes/markevery_line_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/shaped_data.pdf)   (/testbed/result_images/test_axes/polar_wrap_180.pdf)   (/testbed/result_images/test_axes/polar_rmin.pdf)   (/testbed/result_images/test_axes/polar_negative_rmin.pdf)   (/testbed/result_images/test_axes/axvspan_epoch.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   5   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 146
E           GS<5>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
______________________ test_markevery_linear_scales[pdf] _______________________

expected = '/testbed/result_images/test_axes/markevery_linear_scales-expected.pdf'
actual = '/testbed/result_images/test_axes/markevery_linear_scales.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/markevery_linear_scales.pdf'
dest = '/testbed/result_images/test_axes/markevery_linear_scales_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS<4>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
___________________ test_markevery_linear_scales_zoomed[pdf] ___________________

expected = '/testbed/result_images/test_axes/markevery_linear_scales_zoomed-expected.pdf'
actual = '/testbed/result_images/test_axes/markevery_linear_scales_zoomed.pdf'
tol = 0, in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/markevery_linear_scales_zoomed.pdf'
dest = '/testbed/result_images/test_axes/markevery_linear_scales_zoomed_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/shaped_data.pdf)   (/testbed/result_images/test_axes/polar_wrap_180.pdf)   (/testbed/result_images/test_axes/polar_rmin.pdf)   (/testbed/result_images/test_axes/polar_negative_rmin.pdf)   (/testbed/result_images/test_axes/axhspan_epoch.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   5   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 146
E           GS<5>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
________________________ test_markevery_log_scales[pdf] ________________________

expected = '/testbed/result_images/test_axes/markevery_log_scales-expected.pdf'
actual = '/testbed/result_images/test_axes/markevery_log_scales.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/markevery_log_scales.pdf'
dest = '/testbed/result_images/test_axes/markevery_log_scales_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS<4>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
__________________________ test_markevery_polar[pdf] ___________________________

expected = '/testbed/result_images/test_axes/markevery_polar-expected.pdf'
actual = '/testbed/result_images/test_axes/markevery_polar.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/markevery_polar.pdf'
dest = '/testbed/result_images/test_axes/markevery_polar_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS<3>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
____________________________ test_marker_edges[pdf] ____________________________

expected = '/testbed/result_images/test_axes/marker_edges-expected.pdf'
actual = '/testbed/result_images/test_axes/marker_edges.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/marker_edges.pdf'
dest = '/testbed/result_images/test_axes/marker_edges_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/shaped_data.pdf)   (/testbed/result_images/test_axes/polar_wrap_180.pdf)   (/testbed/result_images/test_axes/polar_rmin.pdf)   (/testbed/result_images/test_axes/nonfinite_limits.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   4   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 152
E           GS<4>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_______________________ test_bar_tick_label_single[png] ________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 1.571):
    	result_images/test_axes/bar_tick_label_single.png
    	result_images/test_axes/bar_tick_label_single-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
______________________ test_bar_tick_label_multiple[png] _______________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 1.779):
    	result_images/test_axes/bar_tick_label_multiple.png
    	result_images/test_axes/bar_tick_label_multiple-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_______________ test_bar_tick_label_multiple_old_alignment[png] ________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 1.779):
    	result_images/test_axes/bar_tick_label_multiple_old_label_alignment.png
    	result_images/test_axes/bar_tick_label_multiple_old_label_alignment-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
__________________________ test_barh_tick_label[png] ___________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 4.811):
    	result_images/test_axes/barh_tick_label.png
    	result_images/test_axes/barh_tick_label-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
______________________________ test_hist_log[pdf] ______________________________

expected = '/testbed/result_images/test_axes/hist_log-expected.pdf'
actual = '/testbed/result_images/test_axes/hist_log.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/hist_log.pdf'
dest = '/testbed/result_images/test_axes/hist_log_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS<3>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
____________________________ test_hist_steplog[pdf] ____________________________

expected = '/testbed/result_images/test_axes/hist_steplog-expected.pdf'
actual = '/testbed/result_images/test_axes/hist_steplog.pdf', tol = 0.1
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/hist_steplog.pdf'
dest = '/testbed/result_images/test_axes/hist_steplog_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS<2>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
____________________________ test_hist_density[png] ____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 9.655):
    	result_images/test_axes/hist_density.png
    	result_images/test_axes/hist_density-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
__________________________ test_contour_hatching[pdf] __________________________

expected = '/testbed/result_images/test_axes/contour_hatching-expected.pdf'
actual = '/testbed/result_images/test_axes/contour_hatching.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/contour_hatching.pdf'
dest = '/testbed/result_images/test_axes/contour_hatching_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS<1>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
__________________________ test_contour_colorbar[png] __________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 5.422):
    	result_images/test_axes/contour_colorbar.png
    	result_images/test_axes/contour_colorbar-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
__________________________ test_contour_colorbar[pdf] __________________________

expected = '/testbed/result_images/test_axes/contour_colorbar-expected.pdf'
actual = '/testbed/result_images/test_axes/contour_colorbar.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/contour_colorbar.pdf'
dest = '/testbed/result_images/test_axes/contour_colorbar_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/shaped_data.pdf)   (/testbed/result_images/test_axes/imshow.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   2   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 132
E           GS<2>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_______________________________ test_hist2d[pdf] _______________________________

expected = '/testbed/result_images/test_axes/hist2d-expected.pdf'
actual = '/testbed/result_images/test_axes/hist2d.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/hist2d.pdf'
dest = '/testbed/result_images/test_axes/hist2d_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS<1>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
__________________________ test_hist2d_transpose[pdf] __________________________

expected = '/testbed/result_images/test_axes/hist2d_transpose-expected.pdf'
actual = '/testbed/result_images/test_axes/hist2d_transpose.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/hist2d_transpose.pdf'
dest = '/testbed/result_images/test_axes/hist2d_transpose_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
______________________ TestScatter.test_scatter_plot[pdf] ______________________

expected = '/testbed/result_images/test_axes/scatter-expected.pdf'
actual = '/testbed/result_images/test_axes/scatter.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/scatter.pdf'
dest = '/testbed/result_images/test_axes/scatter_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/imshow_clip.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   1   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 142
E           GS<1>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_____________________________ test_as_mpl_axes_api _____________________________

    def test_as_mpl_axes_api():
        # tests the _as_mpl_axes api
        from matplotlib.projections.polar import PolarAxes
        import matplotlib.axes as maxes
    
        class Polar(object):
            def __init__(self):
                self.theta_offset = 0
    
            def _as_mpl_axes(self):
                # implement the matplotlib axes interface
                return PolarAxes, {'theta_offset': self.theta_offset}
    
        prj = Polar()
        prj2 = Polar()
        prj2.theta_offset = np.pi
        prj3 = Polar()
    
        # testing axes creation with plt.axes
        ax = plt.axes([0, 0, 1, 1], projection=prj)
        assert type(ax) == PolarAxes
        ax_via_gca = plt.gca(projection=prj)
        assert ax_via_gca is ax
        plt.close()
    
        # testing axes creation with gca
        ax = plt.gca(projection=prj)
        assert type(ax) == maxes._subplots.subplot_class_factory(PolarAxes)
        ax_via_gca = plt.gca(projection=prj)
        assert ax_via_gca is ax
        # try getting the axes given a different polar projection
        with pytest.warns(UserWarning) as rec:
            ax_via_gca = plt.gca(projection=prj2)
>           assert len(rec) == 1
E           assert 2 == 1
E            +  where 2 = len(WarningsChecker(record=True))

lib/matplotlib/tests/test_axes.py:2064: AssertionError
_____________________________ test_log_scales[png] _____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 4.401):
    	result_images/test_axes/log_scales.png
    	result_images/test_axes/log_scales-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_____________________________ test_log_scales[pdf] _____________________________

expected = '/testbed/result_images/test_axes/log_scales-expected.pdf'
actual = '/testbed/result_images/test_axes/log_scales.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/log_scales.pdf'
dest = '/testbed/result_images/test_axes/log_scales_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_____________________________ test_stackplot[png] ______________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 5.004):
    	result_images/test_axes/stackplot_test_image.png
    	result_images/test_axes/stackplot_test_image-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_____________________________ test_stackplot[pdf] ______________________________

expected = '/testbed/result_images/test_axes/stackplot_test_image-expected.pdf'
actual = '/testbed/result_images/test_axes/stackplot_test_image.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/stackplot_test_image.pdf'
dest = '/testbed/result_images/test_axes/stackplot_test_image_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /stackunderflow in --pop--
E           Operand stack:
E           
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   %loop_continue   --nostringval--   --nostringval--   false   1   %stopped_push   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Current file position is 4
E           GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_________________________ test_stackplot_baseline[pdf] _________________________

expected = '/testbed/result_images/test_axes/stackplot_test_baseline-expected.pdf'
actual = '/testbed/result_images/test_axes/stackplot_test_baseline.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/stackplot_test_baseline.pdf'
dest = '/testbed/result_images/test_axes/stackplot_test_baseline_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /stackunderflow in --pop--
E           Operand stack:
E           
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   %loop_continue   --nostringval--   --nostringval--   false   1   %stopped_push   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Current file position is 4
E           GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
____________________________ test_bxp_baseline[png] ____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 1.863):
    	result_images/test_axes/bxp_baseline.png
    	result_images/test_axes/bxp_baseline-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
___________________________ test_bxp_rangewhis[png] ____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 1.863):
    	result_images/test_axes/bxp_rangewhis.png
    	result_images/test_axes/bxp_rangewhis-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_________________________ test_bxp_precentilewhis[png] _________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 1.863):
    	result_images/test_axes/bxp_precentilewhis.png
    	result_images/test_axes/bxp_precentilewhis-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
__________________________ test_bxp_with_xlabels[png] __________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 1.943):
    	result_images/test_axes/bxp_with_xlabels.png
    	result_images/test_axes/bxp_with_xlabels-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
__________________________ test_bxp_with_ylabels[png] __________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 1.943):
    	result_images/test_axes/bxp_with_ylabels.png
    	result_images/test_axes/bxp_with_ylabels-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
______________________________ test_boxplot[png] _______________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.753):
    	result_images/test_axes/boxplot.png
    	result_images/test_axes/boxplot-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
______________________________ test_boxplot[pdf] _______________________________

expected = '/testbed/result_images/test_axes/boxplot-expected.pdf'
actual = '/testbed/result_images/test_axes/boxplot.pdf', tol = 1.28
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/boxplot.pdf'
dest = '/testbed/result_images/test_axes/boxplot_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/polycollection_joinstyle.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   1   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 168
E           GS<1>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_____________________ test_boxplot_autorange_whiskers[png] _____________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 2.561):
    	result_images/test_axes/boxplot_autorange_false_whiskers.png
    	result_images/test_axes/boxplot_autorange_false_whiskers-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_______________________ test_boxplot_rc_parameters[pdf] ________________________

expected = '/testbed/result_images/test_axes/boxplot_rc_parameters-expected.pdf'
actual = '/testbed/result_images/test_axes/boxplot_rc_parameters.pdf', tol = 1
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/boxplot_rc_parameters.pdf'
dest = '/testbed/result_images/test_axes/boxplot_rc_parameters_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
______________________ test_vert_violinplot_baseline[png] ______________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 6.913):
    	result_images/test_axes/violinplot_vert_baseline.png
    	result_images/test_axes/violinplot_vert_baseline-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_____________________ test_vert_violinplot_showmeans[png] ______________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 7.104):
    	result_images/test_axes/violinplot_vert_showmeans.png
    	result_images/test_axes/violinplot_vert_showmeans-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
____________________ test_vert_violinplot_showextrema[png] _____________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 6.913):
    	result_images/test_axes/violinplot_vert_showextrema.png
    	result_images/test_axes/violinplot_vert_showextrema-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
____________________ test_vert_violinplot_showmedians[png] _____________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 7.104):
    	result_images/test_axes/violinplot_vert_showmedians.png
    	result_images/test_axes/violinplot_vert_showmedians-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
______________________ test_vert_violinplot_showall[png] _______________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 6.922):
    	result_images/test_axes/violinplot_vert_showall.png
    	result_images/test_axes/violinplot_vert_showall-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
__________________ test_vert_violinplot_custompoints_10[png] ___________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 7.104):
    	result_images/test_axes/violinplot_vert_custompoints_10.png
    	result_images/test_axes/violinplot_vert_custompoints_10-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
__________________ test_vert_violinplot_custompoints_200[png] __________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 7.035):
    	result_images/test_axes/violinplot_vert_custompoints_200.png
    	result_images/test_axes/violinplot_vert_custompoints_200-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_____________________ test_horiz_violinplot_baseline[png] ______________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.985):
    	result_images/test_axes/violinplot_horiz_baseline.png
    	result_images/test_axes/violinplot_horiz_baseline-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
____________________ test_horiz_violinplot_showmedians[png] ____________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.373):
    	result_images/test_axes/violinplot_horiz_showmedians.png
    	result_images/test_axes/violinplot_horiz_showmedians-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_____________________ test_horiz_violinplot_showmeans[png] _____________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.367):
    	result_images/test_axes/violinplot_horiz_showmeans.png
    	result_images/test_axes/violinplot_horiz_showmeans-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
____________________ test_horiz_violinplot_showextrema[png] ____________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.943):
    	result_images/test_axes/violinplot_horiz_showextrema.png
    	result_images/test_axes/violinplot_horiz_showextrema-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
______________________ test_horiz_violinplot_showall[png] ______________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.943):
    	result_images/test_axes/violinplot_horiz_showall.png
    	result_images/test_axes/violinplot_horiz_showall-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
__________________ test_horiz_violinplot_custompoints_10[png] __________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.872):
    	result_images/test_axes/violinplot_horiz_custompoints_10.png
    	result_images/test_axes/violinplot_horiz_custompoints_10-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_________________ test_horiz_violinplot_custompoints_200[png] __________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.872):
    	result_images/test_axes/violinplot_horiz_custompoints_200.png
    	result_images/test_axes/violinplot_horiz_custompoints_200-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
______________________________ test_errorbar[png] ______________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 7.066):
    	result_images/test_axes/errorbar_basic.png
    	result_images/test_axes/errorbar_basic-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
______________________________ test_errorbar[pdf] ______________________________

expected = '/testbed/result_images/test_axes/errorbar_basic-expected.pdf'
actual = '/testbed/result_images/test_axes/errorbar_basic.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/errorbar_basic.pdf'
dest = '/testbed/result_images/test_axes/errorbar_basic_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /stackunderflow in --pop--
E           Operand stack:
E           
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   %loop_continue   --nostringval--   --nostringval--   false   1   %stopped_push   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Current file position is 4
E           GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
__________________________ test_errorbar_limits[png] ___________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.694):
    	result_images/test_axes/errorbar_limits.png
    	result_images/test_axes/errorbar_limits-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
__________________________ test_errorbar_limits[pdf] ___________________________

expected = '/testbed/result_images/test_axes/errorbar_limits-expected.pdf'
actual = '/testbed/result_images/test_axes/errorbar_limits.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/errorbar_limits.pdf'
dest = '/testbed/result_images/test_axes/errorbar_limits_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /stackunderflow in --pop--
E           Operand stack:
E           
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   %loop_continue   --nostringval--   --nostringval--   false   1   %stopped_push   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Current file position is 4
E           GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
______________________ test_hist_stacked_stepfilled[png] _______________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.276):
    	result_images/test_axes/hist_stacked_stepfilled.png
    	result_images/test_axes/hist_stacked_stepfilled-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
______________________ test_hist_stacked_stepfilled[pdf] _______________________

expected = '/testbed/result_images/test_axes/hist_stacked_stepfilled-expected.pdf'
actual = '/testbed/result_images/test_axes/hist_stacked_stepfilled.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/hist_stacked_stepfilled.pdf'
dest = '/testbed/result_images/test_axes/hist_stacked_stepfilled_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /stackunderflow in --pop--
E           Operand stack:
E           
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   %loop_continue   --nostringval--   --nostringval--   false   1   %stopped_push   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Current file position is 4
E           GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
____________________________ test_hist_offset[png] _____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 4.051):
    	result_images/test_axes/hist_offset.png
    	result_images/test_axes/hist_offset-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
____________________________ test_hist_offset[pdf] _____________________________

expected = '/testbed/result_images/test_axes/hist_offset-expected.pdf'
actual = '/testbed/result_images/test_axes/hist_offset.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/hist_offset.pdf'
dest = '/testbed/result_images/test_axes/hist_offset_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/fill_between_interpolate.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   1   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 168
E           GS<1>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
__________________________ test_hist_step_horiz[png] ___________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.219):
    	result_images/test_axes/hist_step_horiz.png
    	result_images/test_axes/hist_step_horiz-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_______________________ test_hist_stacked_weighted[png] ________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.298):
    	result_images/test_axes/hist_stacked_weights.png
    	result_images/test_axes/hist_stacked_weights-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_______________________ test_hist_stacked_weighted[pdf] ________________________

expected = '/testbed/result_images/test_axes/hist_stacked_weights-expected.pdf'
actual = '/testbed/result_images/test_axes/hist_stacked_weights.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/hist_stacked_weights.pdf'
dest = '/testbed/result_images/test_axes/hist_stacked_weights_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
___________________ test_hist_stacked_stepfilled_alpha[png] ____________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.276):
    	result_images/test_axes/hist_stacked_stepfilled_alpha.png
    	result_images/test_axes/hist_stacked_stepfilled_alpha-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
___________________ test_hist_stacked_stepfilled_alpha[pdf] ____________________

expected = '/testbed/result_images/test_axes/hist_stacked_stepfilled_alpha-expected.pdf'
actual = '/testbed/result_images/test_axes/hist_stacked_stepfilled_alpha.pdf'
tol = 0, in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/hist_stacked_stepfilled_alpha.pdf'
dest = '/testbed/result_images/test_axes/hist_stacked_stepfilled_alpha_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /stackunderflow in --pop--
E           Operand stack:
E           
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   %loop_continue   --nostringval--   --nostringval--   false   1   %stopped_push   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Current file position is 4
E           GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_________________________ test_hist_stacked_step[png] __________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.276):
    	result_images/test_axes/hist_stacked_step.png
    	result_images/test_axes/hist_stacked_step-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_________________________ test_hist_stacked_step[pdf] __________________________

expected = '/testbed/result_images/test_axes/hist_stacked_step-expected.pdf'
actual = '/testbed/result_images/test_axes/hist_stacked_step.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/hist_stacked_step.pdf'
dest = '/testbed/result_images/test_axes/hist_stacked_step_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /stackunderflow in --pop--
E           Operand stack:
E           
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   %loop_continue   --nostringval--   --nostringval--   false   1   %stopped_push   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Current file position is 4
E           GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
________________________ test_hist_stacked_density[png] ________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 7.917):
    	result_images/test_axes/hist_stacked_normed.png
    	result_images/test_axes/hist_stacked_normed-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
________________________ test_hist_stacked_density[pdf] ________________________

expected = '/testbed/result_images/test_axes/hist_stacked_normed-expected.pdf'
actual = '/testbed/result_images/test_axes/hist_stacked_normed.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/hist_stacked_normed.pdf'
dest = '/testbed/result_images/test_axes/hist_stacked_normed_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /stackunderflow in --pop--
E           Operand stack:
E           
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   %loop_continue   --nostringval--   --nostringval--   false   1   %stopped_push   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Current file position is 4
E           GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
__________________________ test_hist_stacked_bar[png] __________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 11.231):
    	result_images/test_axes/hist_stacked_bar.png
    	result_images/test_axes/hist_stacked_bar-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
__________________________ test_hist_stacked_bar[pdf] __________________________

expected = '/testbed/result_images/test_axes/hist_stacked_bar-expected.pdf'
actual = '/testbed/result_images/test_axes/hist_stacked_bar.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/hist_stacked_bar.pdf'
dest = '/testbed/result_images/test_axes/hist_stacked_bar_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /stackunderflow in --pop--
E           Operand stack:
E           
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   %loop_continue   --nostringval--   --nostringval--   false   1   %stopped_push   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Current file position is 4
E           GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
________________________ test_transparent_markers[pdf] _________________________

expected = '/testbed/result_images/test_axes/transparent_markers-expected.pdf'
actual = '/testbed/result_images/test_axes/transparent_markers.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/transparent_markers.pdf'
dest = '/testbed/result_images/test_axes/transparent_markers_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/fill_between_interpolate_decreasing.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   1   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 190
E           GS<1>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
____________________________ test_rgba_markers[pdf] ____________________________

expected = '/testbed/result_images/test_axes/rgba_markers-expected.pdf'
actual = '/testbed/result_images/test_axes/rgba_markers.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/rgba_markers.pdf'
dest = '/testbed/result_images/test_axes/rgba_markers_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
___________________________ test_mollweide_grid[pdf] ___________________________

expected = '/testbed/result_images/test_axes/mollweide_grid-expected.pdf'
actual = '/testbed/result_images/test_axes/mollweide_grid.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/mollweide_grid.pdf'
dest = '/testbed/result_images/test_axes/mollweide_grid_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /stackunderflow in --pop--
E           Operand stack:
E           
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   %loop_continue   --nostringval--   --nostringval--   false   1   %stopped_push   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Current file position is 4
E           GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_______________________________ test_alpha[pdf] ________________________________

expected = '/testbed/result_images/test_axes/test_alpha-expected.pdf'
actual = '/testbed/result_images/test_axes/test_alpha.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/test_alpha.pdf'
dest = '/testbed/result_images/test_axes/test_alpha_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /stackunderflow in --pop--
E           Operand stack:
E           
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   %loop_continue   --nostringval--   --nostringval--   false   1   %stopped_push   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Current file position is 4
E           GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_____________________________ test_eventplot[pdf] ______________________________

expected = '/testbed/result_images/test_axes/eventplot-expected.pdf'
actual = '/testbed/result_images/test_axes/eventplot.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/eventplot.pdf'
dest = '/testbed/result_images/test_axes/eventplot_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /stackunderflow in --pop--
E           Operand stack:
E           
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   %loop_continue   --nostringval--   --nostringval--   false   1   %stopped_push   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Current file position is 4
E           GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_____________________ test_markers_fillstyle_rcparams[png] _____________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.140):
    	result_images/test_axes/rc_markerfill.png
    	result_images/test_axes/rc_markerfill-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
___________________________ test_eb_line_zorder[png] ___________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 4.086):
    	result_images/test_axes/vline_hline_zorder.png
    	result_images/test_axes/vline_hline_zorder-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
___________________________ test_eb_line_zorder[pdf] ___________________________

expected = '/testbed/result_images/test_axes/vline_hline_zorder-expected.pdf'
actual = '/testbed/result_images/test_axes/vline_hline_zorder.pdf', tol = 0.0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/vline_hline_zorder.pdf'
dest = '/testbed/result_images/test_axes/vline_hline_zorder_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /invalidfileaccess in --run--
E           Operand stack:
E              (/testbed/result_images/test_axes/symlog.pdf)   (r)
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   run   %loop_continue   run   run   false   1   %stopped_push   .runexec2   --nostringval--   run   --nostringval--   2   %stopped_push   --nostringval--   1990   1   3   %oparray_pop   run
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Last OS error: Permission denied
E           Current file position is 132
E           GS<1>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
_______________________________ test_vlines[png] _______________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.489):
    	result_images/test_axes/vlines_basic.png
    	result_images/test_axes/vlines_basic-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_______________________________ test_hlines[png] _______________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.667):
    	result_images/test_axes/hlines_basic.png
    	result_images/test_axes/hlines_basic-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
___________________________ test_step_linestyle[pdf] ___________________________

expected = '/testbed/result_images/test_axes/step_linestyle-expected.pdf'
actual = '/testbed/result_images/test_axes/step_linestyle.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/step_linestyle.pdf'
dest = '/testbed/result_images/test_axes/step_linestyle_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
__________________________ test_mixed_collection[pdf] __________________________

expected = '/testbed/result_images/test_axes/mixed_collection-expected.pdf'
actual = '/testbed/result_images/test_axes/mixed_collection.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/mixed_collection.pdf'
dest = '/testbed/result_images/test_axes/mixed_collection_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /stackunderflow in --pop--
E           Operand stack:
E           
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   %loop_continue   --nostringval--   --nostringval--   false   1   %stopped_push   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Current file position is 4
E           GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
____________________________ test_pie_default[png] _____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 1.403):
    	result_images/test_axes/pie_default.png
    	result_images/test_axes/pie_default-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
__________________________ test_pie_linewidth_0[png] ___________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 4.151):
    	result_images/test_axes/pie_linewidth_0.png
    	result_images/test_axes/pie_linewidth_0-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_________________________ test_pie_center_radius[png] __________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.416):
    	result_images/test_axes/pie_center_radius.png
    	result_images/test_axes/pie_center_radius-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
__________________________ test_pie_linewidth_2[png] ___________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 4.151):
    	result_images/test_axes/pie_linewidth_2.png
    	result_images/test_axes/pie_linewidth_2-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
____________________________ test_pie_ccw_true[png] ____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 4.151):
    	result_images/test_axes/pie_ccw_true.png
    	result_images/test_axes/pie_ccw_true-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
___________________________ test_pie_frame_grid[png] ___________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 6.195):
    	result_images/test_axes/pie_frame_grid.png
    	result_images/test_axes/pie_frame_grid-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_______________________ test_pie_rotatelabels_true[png] ________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 2.687):
    	result_images/test_axes/pie_rotatelabels_true.png
    	result_images/test_axes/pie_rotatelabels_true-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_______________________ test_pie_nolabel_but_legend[png] _______________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 1.028):
    	result_images/test_axes/pie_no_label.png
    	result_images/test_axes/pie_no_label-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_________________________ test_set_get_ticklabels[png] _________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 3.217):
    	result_images/test_axes/set_get_ticklabels.png
    	result_images/test_axes/set_get_ticklabels-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_______________________ test_retain_tick_visibility[png] _______________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 4.560):
    	result_images/test_axes/retain_tick_visibility.png
    	result_images/test_axes/retain_tick_visibility-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_________________________ test_o_marker_path_snap[png] _________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 0.008):
    	result_images/test_axes/o_marker_path_snap.png
    	result_images/test_axes/o_marker_path_snap-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_____________________________ test_rc_spines[png] ______________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 8.732):
    	result_images/test_axes/rc_spines.png
    	result_images/test_axes/rc_spines-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
______________________________ test_rc_grid[png] _______________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 13.966):
    	result_images/test_axes/rc_grid.png
    	result_images/test_axes/rc_grid-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
____________________________ test_dash_offset[pdf] _____________________________

expected = '/testbed/result_images/test_axes/dash_offset-expected.pdf'
actual = '/testbed/result_images/test_axes/dash_offset.pdf', tol = 0
in_decorator = True

    def compare_images(expected, actual, tol, in_decorator=False):
        """
        Compare two "image" files checking differences within a tolerance.
    
        The two given filenames may point to files which are convertible to
        PNG via the `.converter` dictionary. The underlying RMS is calculated
        with the `.calculate_rms` function.
    
        Parameters
        ----------
        expected : str
            The filename of the expected image.
        actual : str
            The filename of the actual image.
        tol : float
            The tolerance (a color value difference, where 255 is the
            maximal difference).  The test fails if the average pixel
            difference is greater than this value.
        in_decorator : bool
            Determines the output format. If called from image_comparison
            decorator, this should be True. (default=False)
    
        Returns
        -------
        comparison_result : None or dict or str
            Return *None* if the images are equal within the given tolerance.
    
            If the images differ, the return value depends on  *in_decorator*.
            If *in_decorator* is true, a dict with the following entries is
            returned:
    
            - *rms*: The RMS of the image difference.
            - *expected*: The filename of the expected image.
            - *actual*: The filename of the actual image.
            - *diff_image*: The filename of the difference image.
            - *tol*: The comparison tolerance.
    
            Otherwise, a human-readable multi-line string representation of this
            information is returned.
    
        Examples
        --------
        ::
    
            img1 = "./baseline/plot.png"
            img2 = "./output/plot.png"
            compare_images(img1, img2, 0.001)
    
        """
        from matplotlib import _png
    
        if not os.path.exists(actual):
            raise Exception("Output image %s does not exist." % actual)
    
        if os.stat(actual).st_size == 0:
            raise Exception("Output image file %s is empty." % actual)
    
        # Convert the image to png
        extension = expected.split('.')[-1]
    
        if not os.path.exists(expected):
            raise IOError('Baseline image %r does not exist.' % expected)
    
        if extension != 'png':
>           actual = convert(actual, False)

lib/matplotlib/testing/compare.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
lib/matplotlib/testing/compare.py:341: in convert
    converter[extension](filename, newname)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <matplotlib.testing.compare._GSConverter object at 0x2aaadaff8490>
orig = '/testbed/result_images/test_axes/dash_offset.pdf'
dest = '/testbed/result_images/test_axes/dash_offset_pdf.png'

    def __call__(self, orig, dest):
        if not self._proc:
            self._proc = subprocess.Popen(
                [mpl._get_executable_info("gs").executable,
                 "-dNOPAUSE", "-sDEVICE=png16m"],
                # As far as I can see, ghostscript never outputs to stderr.
                stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            try:
                self._read_until(b"\nGS")
            except _ConverterError:
                raise OSError("Failed to start Ghostscript")
    
        def encode_and_escape(name):
            return (os.fsencode(name)
                    .replace(b"\\", b"\\\\")
                    .replace(b"(", br"\(")
                    .replace(b")", br"\)"))
    
        self._proc.stdin.write(
            b"<< /OutputFile ("
            + encode_and_escape(dest)
            + b") >> setpagedevice ("
            + encode_and_escape(orig)
            + b") run flush\n")
        self._proc.stdin.flush()
        # GS> if nothing left on the stack; GS<n> if n items left on the stack.
        err = self._read_until(b"GS")
        stack = self._read_until(b">")
        if stack or not os.path.exists(dest):
            stack_size = int(stack[1:]) if stack else 0
            self._proc.stdin.write(b"pop\n" * stack_size)
            # Using the systemencoding should at least get the filenames right.
            raise ImageComparisonFailure(
                (err + b"GS" + stack + b">")
>               .decode(sys.getfilesystemencoding(), "replace"))
E           matplotlib.testing.exceptions.ImageComparisonFailure: Error: /stackunderflow in --pop--
E           Operand stack:
E           
E           Execution stack:
E              %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   %loop_continue   --nostringval--   --nostringval--   false   1   %stopped_push   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--
E           Dictionary stack:
E              --dict:762/1123(ro)(G)--   --dict:0/20(G)--   --dict:75/200(L)--
E           Current allocation mode is local
E           Current file position is 4
E           GS>

lib/matplotlib/testing/compare.py:208: ImageComparisonFailure
___________________________ test_auto_numticks[png] ____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 12.400):
    	result_images/test_axes/auto_numticks.png
    	result_images/test_axes/auto_numticks-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_________________________ test_auto_numticks_log[png] __________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 7.453):
    	result_images/test_axes/auto_numticks_log.png
    	result_images/test_axes/auto_numticks_log-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
__________________________ test_date_timezone_x[png] ___________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 8.541):
    	result_images/test_axes/date_timezone_x.png
    	result_images/test_axes/date_timezone_x-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
__________________________ test_date_timezone_y[png] ___________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 6.001):
    	result_images/test_axes/date_timezone_y.png
    	result_images/test_axes/date_timezone_y-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_______________________ test_date_timezone_x_and_y[png] ________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 10.737):
    	result_images/test_axes/date_timezone_x_and_y.png
    	result_images/test_axes/date_timezone_x_and_y-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_____________________________ test_titletwiny[png] _____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 5.004):
    	result_images/test_axes/titletwiny.png
    	result_images/test_axes/titletwiny-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
____________________________ test_secondary_xy[png] ____________________________

E   matplotlib.testing.exceptions.ImageComparisonFailure: images not close (RMS 1.142):
    	result_images/test_axes/secondary_xy.png
    	result_images/test_axes/secondary_xy-expected.png
All traceback entries are hidden. Pass `--full-trace` to see hidden and internal frames.
_______________________________ test_normal_axes _______________________________

    def test_normal_axes():
        with rc_context({'_internal.classic_mode': False}):
            fig, ax = plt.subplots(dpi=200, figsize=(6, 6))
            fig.canvas.draw()
            plt.close(fig)
            bbaxis, bbspines, bbax, bbtb = color_boxes(fig, ax)
    
        # test the axis bboxes
        target = [
            [123.375, 75.88888888888886, 983.25, 33.0],
            [85.51388888888889, 99.99999999999997, 53.375, 993.0]
        ]
        for nn, b in enumerate(bbaxis):
            targetbb = mtransforms.Bbox.from_bounds(*target[nn])
>           assert_array_almost_equal(b.bounds, targetbb.bounds, decimal=2)
E           AssertionError: 
E           Arrays are not almost equal to 2 decimals
E           
E           Mismatched elements: 2 / 4 (50%)
E           Max absolute difference: 0.125
E           Max relative difference: 0.00050659
E            x: array([123.44,  75.89, 983.12,  33.  ])
E            y: array([123.38,  75.89, 983.25,  33.  ])

lib/matplotlib/tests/test_axes.py:6183: AssertionError
___________________________ test_get_tightbbox_polar ___________________________

    def test_get_tightbbox_polar():
        fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})
        fig.canvas.draw()
        bb = ax.get_tightbbox(fig.canvas.get_renderer())
        assert_allclose(bb.extents,
>           [107.7778,  29.2778, 539.7847, 450.7222], rtol=1e-03)
E       AssertionError: 
E       Not equal to tolerance rtol=0.001, atol=0
E       
E       Mismatched elements: 2 / 4 (50%)
E       Max absolute difference: 0.50002222
E       Max relative difference: 0.01707854
E        x: array([107.715278,  28.777778, 539.784722, 451.222222])
E        y: array([107.7778,  29.2778, 539.7847, 450.7222])

lib/matplotlib/tests/test_axes.py:6326: AssertionError
=============================== warnings summary ===============================
lib/matplotlib/__init__.py:195
lib/matplotlib/__init__.py:195
lib/matplotlib/__init__.py:195
lib/matplotlib/__init__.py:195
lib/matplotlib/__init__.py:195
  /testbed/lib/matplotlib/__init__.py:195: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(module.__version__) < minver:

../opt/miniconda3/envs/testbed/lib/python3.7/site-packages/setuptools/_distutils/version.py:346
../opt/miniconda3/envs/testbed/lib/python3.7/site-packages/setuptools/_distutils/version.py:346
../opt/miniconda3/envs/testbed/lib/python3.7/site-packages/setuptools/_distutils/version.py:346
../opt/miniconda3/envs/testbed/lib/python3.7/site-packages/setuptools/_distutils/version.py:346
../opt/miniconda3/envs/testbed/lib/python3.7/site-packages/setuptools/_distutils/version.py:346
../opt/miniconda3/envs/testbed/lib/python3.7/site-packages/setuptools/_distutils/version.py:346
  /opt/miniconda3/envs/testbed/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    other = LooseVersion(other)

lib/matplotlib/__init__.py:321
  /testbed/lib/matplotlib/__init__.py:321: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    version = LooseVersion(match.group(1))

lib/matplotlib/testing/decorators.py:313
  /testbed/lib/matplotlib/testing/decorators.py:313: PytestUnknownMarkWarning: Unknown pytest.mark.baseline_images - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.baseline_images(baseline_images)

lib/matplotlib/tests/test_axes.py: 86 warnings
  /testbed/lib/matplotlib/projections/polar.py:723: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    np.array([[0.0, 0.0], [1.0, 1.0]], np.float),

lib/matplotlib/tests/test_axes.py::test_shaped_data[png]
lib/matplotlib/tests/test_axes.py::test_shaped_data[pdf]
  /opt/miniconda3/envs/testbed/lib/python3.7/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
    ary = asanyarray(ary)

lib/matplotlib/tests/test_axes.py::test_pcolormesh[pdf]
  /testbed/lib/matplotlib/backends/backend_pdf.py:1301: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.
    self.write(streamarr.tostring())

lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_marker[png]
lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case26-None]
lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case27-shape]
lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case28-None]
lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case29-shape]
lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case30-conversion]
lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case31-conversion]
lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case32-conversion]
lib/matplotlib/tests/test_axes.py::test_eventplot_colors[colors2]
  /testbed/lib/matplotlib/colors.py:239: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
    c = np.array(c)

lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_marker[png]
  /testbed/lib/matplotlib/cbook/__init__.py:1120: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
    x = np.asanyarray(x)

lib/matplotlib/tests/test_axes.py::test_boxplot_bad_ci_2
  /opt/miniconda3/envs/testbed/lib/python3.7/site-packages/numpy/core/fromnumeric.py:1970: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
    result = asarray(a).shape

lib/matplotlib/tests/test_axes.py: 15 warnings
  /opt/miniconda3/envs/testbed/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3208: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
    return asarray(a).size

lib/matplotlib/tests/test_axes.py: 15 warnings
  /testbed/lib/matplotlib/cbook/__init__.py:1420: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
    X = np.atleast_1d(X.T if isinstance(X, np.ndarray) else np.asarray(X))

lib/matplotlib/tests/test_axes.py::test_square_plot
  /testbed/lib/matplotlib/transforms.py:907: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.
    self._points[:, 0] = interval

lib/matplotlib/tests/test_axes.py::test_square_plot
  /testbed/lib/matplotlib/transforms.py:912: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.
    self._points[:, 1] = interval

lib/matplotlib/tests/test_axes.py::test_pcolor_fast_RGB
  /testbed/lib/matplotlib/tests/test_axes.py:5212: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    x = np.arange(11, dtype=np.float)

lib/matplotlib/tests/test_axes.py::test_pcolor_fast_RGB
  /testbed/lib/matplotlib/tests/test_axes.py:5213: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    y = np.arange(11, dtype=np.float)

lib/matplotlib/tests/test_axes.py::test_secondary_xy[png]
  /testbed/lib/matplotlib/tests/test_axes.py:6098: RuntimeWarning: invalid value encountered in sqrt
    axsec = secax(0.6, functions=(lambda x: x**2, lambda x: x**(1/2)))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
==================================== PASSES ====================================
__________________ TestScatter.test_scatter_c[c_case12-None] ___________________
------------------------------ Captured log call -------------------------------
WARNING  matplotlib.axes._axes:_axes.py:4250 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
__________________________ test_color_length_mismatch __________________________
------------------------------ Captured log call -------------------------------
WARNING  matplotlib.axes._axes:_axes.py:4250 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
=========================== short test summary info ============================
PASSED lib/matplotlib/tests/test_axes.py::test_get_labels
PASSED lib/matplotlib/tests/test_axes.py::test_spy_invalid_kwargs
PASSED lib/matplotlib/tests/test_axes.py::test_twinx_cla
PASSED lib/matplotlib/tests/test_axes.py::test_twinx_axis_scales[png]
PASSED lib/matplotlib/tests/test_axes.py::test_twin_inherit_autoscale_setting
PASSED lib/matplotlib/tests/test_axes.py::test_inverted_cla
PASSED lib/matplotlib/tests/test_axes.py::test_minorticks_on_rcParams_both[png]
PASSED lib/matplotlib/tests/test_axes.py::test_autoscale_tiny_range[png]
PASSED lib/matplotlib/tests/test_axes.py::test_autoscale_tight
PASSED lib/matplotlib/tests/test_axes.py::test_autoscale_log_shared
PASSED lib/matplotlib/tests/test_axes.py::test_use_sticky_edges
PASSED lib/matplotlib/tests/test_axes.py::test_arrow_simple[png]
PASSED lib/matplotlib/tests/test_axes.py::test_arrow_empty
PASSED lib/matplotlib/tests/test_axes.py::test_annotate_default_arrow
PASSED lib/matplotlib/tests/test_axes.py::test_structured_data
PASSED lib/matplotlib/tests/test_axes.py::test_polar_rlim[png]
PASSED lib/matplotlib/tests/test_axes.py::test_polar_rlim_bottom[png]
PASSED lib/matplotlib/tests/test_axes.py::test_hexbin_extent[png]
PASSED lib/matplotlib/tests/test_axes.py::test_hexbin_empty[png]
PASSED lib/matplotlib/tests/test_axes.py::test_hexbin_pickable
PASSED lib/matplotlib/tests/test_axes.py::test_inverted_limits
PASSED lib/matplotlib/tests/test_axes.py::test_imshow[png]
PASSED lib/matplotlib/tests/test_axes.py::test_polycollection_joinstyle[png]
PASSED lib/matplotlib/tests/test_axes.py::test_fill_between_input[2d_x_input]
PASSED lib/matplotlib/tests/test_axes.py::test_fill_between_input[2d_y1_input]
PASSED lib/matplotlib/tests/test_axes.py::test_fill_between_input[2d_y2_input]
PASSED lib/matplotlib/tests/test_axes.py::test_fill_betweenx_input[2d_y_input]
PASSED lib/matplotlib/tests/test_axes.py::test_fill_betweenx_input[2d_x1_input]
PASSED lib/matplotlib/tests/test_axes.py::test_fill_betweenx_input[2d_x2_input]
PASSED lib/matplotlib/tests/test_axes.py::test_fill_between_interpolate[png]
PASSED lib/matplotlib/tests/test_axes.py::test_fill_between_interpolate_decreasing[png]
PASSED lib/matplotlib/tests/test_axes.py::test_pcolorargs_5205
PASSED lib/matplotlib/tests/test_axes.py::test_pcolormesh[png]
PASSED lib/matplotlib/tests/test_axes.py::test_pcolorargs
PASSED lib/matplotlib/tests/test_axes.py::test_arc_angles[png]
PASSED lib/matplotlib/tests/test_axes.py::test_arc_ellipse[png]
PASSED lib/matplotlib/tests/test_axes.py::test_markevery_linear_scales[png]
PASSED lib/matplotlib/tests/test_axes.py::test_markevery_linear_scales_zoomed[png]
PASSED lib/matplotlib/tests/test_axes.py::test_markevery_log_scales[png]
PASSED lib/matplotlib/tests/test_axes.py::test_markevery_polar[png]
PASSED lib/matplotlib/tests/test_axes.py::test_marker_edges[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bar_ticklabel_fail
PASSED lib/matplotlib/tests/test_axes.py::test_bar_color_none_alpha
PASSED lib/matplotlib/tests/test_axes.py::test_bar_edgecolor_none_alpha
PASSED lib/matplotlib/tests/test_axes.py::test_bar_timedelta
PASSED lib/matplotlib/tests/test_axes.py::test_hist_log[png]
PASSED lib/matplotlib/tests/test_axes.py::test_hist_bar_empty[png]
PASSED lib/matplotlib/tests/test_axes.py::test_hist_step_empty[png]
PASSED lib/matplotlib/tests/test_axes.py::test_hist_steplog[png]
PASSED lib/matplotlib/tests/test_axes.py::test_hist_step_filled[png]
PASSED lib/matplotlib/tests/test_axes.py::test_hist_step_log_bottom[png]
PASSED lib/matplotlib/tests/test_axes.py::test_hist_unequal_bins_density
PASSED lib/matplotlib/tests/test_axes.py::test_hist_datetime_datasets
PASSED lib/matplotlib/tests/test_axes.py::test_hist_with_empty_input[data0-1]
PASSED lib/matplotlib/tests/test_axes.py::test_hist_with_empty_input[data1-1]
PASSED lib/matplotlib/tests/test_axes.py::test_hist_with_empty_input[data2-2]
PASSED lib/matplotlib/tests/test_axes.py::test_contour_hatching[png]
PASSED lib/matplotlib/tests/test_axes.py::test_hist2d[png]
PASSED lib/matplotlib/tests/test_axes.py::test_hist2d_transpose[png]
PASSED lib/matplotlib/tests/test_axes.py::test_hist2d_density_normed
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_plot[png]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_marker[png]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_2D[png]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_color
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_invalid_color[png]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_no_invalid_color[png]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[0.5-None]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[rgby-None]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[rgb-shape]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[rgbrgb-shape]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case4-conversion]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[red-None]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[none-None]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[None-None]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case8-None]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[jaune-conversion]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case10-conversion]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case11-conversion]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case12-None]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case13-None]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case14-shape]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case15-None]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case16-None]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case17-conversion]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case18-None]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case19-shape]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case20-None]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case21-shape]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case22-None]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case23-shape]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case24-None]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case25-shape]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case26-None]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case27-shape]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case28-None]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case29-shape]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case30-conversion]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case31-conversion]
PASSED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_c[c_case32-conversion]
PASSED lib/matplotlib/tests/test_axes.py::test_parse_scatter_color_args[params0-expected_result0]
PASSED lib/matplotlib/tests/test_axes.py::test_parse_scatter_color_args[params1-expected_result1]
PASSED lib/matplotlib/tests/test_axes.py::test_parse_scatter_color_args[params2-expected_result2]
PASSED lib/matplotlib/tests/test_axes.py::test_parse_scatter_color_args[params3-expected_result3]
PASSED lib/matplotlib/tests/test_axes.py::test_parse_scatter_color_args[params4-expected_result4]
PASSED lib/matplotlib/tests/test_axes.py::test_parse_scatter_color_args_edgecolors[kwargs0-None]
PASSED lib/matplotlib/tests/test_axes.py::test_parse_scatter_color_args_edgecolors[kwargs1-None]
PASSED lib/matplotlib/tests/test_axes.py::test_parse_scatter_color_args_edgecolors[kwargs2-r]
PASSED lib/matplotlib/tests/test_axes.py::test_parse_scatter_color_args_edgecolors[kwargs3-expected_edgecolors3]
PASSED lib/matplotlib/tests/test_axes.py::test_parse_scatter_color_args_edgecolors[kwargs4-r]
PASSED lib/matplotlib/tests/test_axes.py::test_parse_scatter_color_args_edgecolors[kwargs5-face]
PASSED lib/matplotlib/tests/test_axes.py::test_parse_scatter_color_args_edgecolors[kwargs6-none]
PASSED lib/matplotlib/tests/test_axes.py::test_parse_scatter_color_args_edgecolors[kwargs7-r]
PASSED lib/matplotlib/tests/test_axes.py::test_parse_scatter_color_args_edgecolors[kwargs8-r]
PASSED lib/matplotlib/tests/test_axes.py::test_parse_scatter_color_args_edgecolors[kwargs9-r]
PASSED lib/matplotlib/tests/test_axes.py::test_parse_scatter_color_args_edgecolors[kwargs10-g]
PASSED lib/matplotlib/tests/test_axes.py::test_pyplot_axes
PASSED lib/matplotlib/tests/test_axes.py::test_stackplot_baseline[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_horizontal[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_patchartist[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_custompatchartist[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_customoutlier[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_showcustommean[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_custombox[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_custommedian[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_customcap[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_customwhisker[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_shownotches[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_nocaps[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_nobox[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_no_flier_stats[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_showmean[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_showmeanasline[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_scalarwidth[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_customwidths[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_custompositions[png]
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_bad_widths
PASSED lib/matplotlib/tests/test_axes.py::test_bxp_bad_positions
PASSED lib/matplotlib/tests/test_axes.py::test_boxplot_sym2[png]
PASSED lib/matplotlib/tests/test_axes.py::test_boxplot_sym[png]
PASSED lib/matplotlib/tests/test_axes.py::test_boxplot_rc_parameters[png]
PASSED lib/matplotlib/tests/test_axes.py::test_boxplot_with_CIarray[png]
PASSED lib/matplotlib/tests/test_axes.py::test_boxplot_no_weird_whisker[png]
PASSED lib/matplotlib/tests/test_axes.py::test_boxplot_bad_medians_1
PASSED lib/matplotlib/tests/test_axes.py::test_boxplot_bad_medians_2
PASSED lib/matplotlib/tests/test_axes.py::test_boxplot_bad_ci_1
PASSED lib/matplotlib/tests/test_axes.py::test_boxplot_zorder
PASSED lib/matplotlib/tests/test_axes.py::test_boxplot_bad_ci_2
PASSED lib/matplotlib/tests/test_axes.py::test_boxplot_mod_artist_after_plotting[png]
PASSED lib/matplotlib/tests/test_axes.py::test_violinplot_bad_positions
PASSED lib/matplotlib/tests/test_axes.py::test_violinplot_bad_widths
PASSED lib/matplotlib/tests/test_axes.py::test_manage_xticks
PASSED lib/matplotlib/tests/test_axes.py::test_boxplot_not_single
PASSED lib/matplotlib/tests/test_axes.py::test_tick_space_size_0
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_colorcycle
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_shape
PASSED lib/matplotlib/tests/test_axes.py::test_errobar_nonefmt
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_with_prop_cycle[png]
PASSED lib/matplotlib/tests/test_axes.py::test_hist_step[png]
PASSED lib/matplotlib/tests/test_axes.py::test_stem[png-w/ line collection]
PASSED lib/matplotlib/tests/test_axes.py::test_stem[png-w/o line collection]
PASSED lib/matplotlib/tests/test_axes.py::test_stem_params[png]
PASSED lib/matplotlib/tests/test_axes.py::test_stem_args
PASSED lib/matplotlib/tests/test_axes.py::test_stem_dates
PASSED lib/matplotlib/tests/test_axes.py::test_hist_normed_density[False-False]
PASSED lib/matplotlib/tests/test_axes.py::test_hist_normed_density[False-True]
PASSED lib/matplotlib/tests/test_axes.py::test_hist_normed_density[True-False]
PASSED lib/matplotlib/tests/test_axes.py::test_hist_normed_density[True-True]
PASSED lib/matplotlib/tests/test_axes.py::test_hist_step_bottom[png]
PASSED lib/matplotlib/tests/test_axes.py::test_hist_emptydata
PASSED lib/matplotlib/tests/test_axes.py::test_hist_labels
PASSED lib/matplotlib/tests/test_axes.py::test_transparent_markers[png]
PASSED lib/matplotlib/tests/test_axes.py::test_rgba_markers[png]
PASSED lib/matplotlib/tests/test_axes.py::test_mollweide_grid[png]
PASSED lib/matplotlib/tests/test_axes.py::test_mollweide_forward_inverse_closure
PASSED lib/matplotlib/tests/test_axes.py::test_mollweide_inverse_forward_closure
PASSED lib/matplotlib/tests/test_axes.py::test_alpha[png]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot[png]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_defaults[png]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_colors[colors0]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_colors[colors1]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_colors[colors2]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_colors[colors3]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_problem_kwargs[png]
PASSED lib/matplotlib/tests/test_axes.py::test_empty_eventplot
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_orientation[data0-_empty]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_orientation[data1-vertical]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_orientation[data2-horizontal]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_orientation[data3-None]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_orientation[data4-none]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_orientation[data5-_empty]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_orientation[data6-vertical]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_orientation[data7-horizontal]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_orientation[data8-None]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_orientation[data9-none]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_orientation[data10-_empty]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_orientation[data11-vertical]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_orientation[data12-horizontal]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_orientation[data13-None]
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_orientation[data14-none]
PASSED lib/matplotlib/tests/test_axes.py::test_marker_styles[png]
PASSED lib/matplotlib/tests/test_axes.py::test_vertex_markers[png]
PASSED lib/matplotlib/tests/test_axes.py::test_step_linestyle[png]
PASSED lib/matplotlib/tests/test_axes.py::test_mixed_collection[png]
PASSED lib/matplotlib/tests/test_axes.py::test_subplot_key_hash
PASSED lib/matplotlib/tests/test_axes.py::test_specgram_freqs[png]
PASSED lib/matplotlib/tests/test_axes.py::test_specgram_noise[png]
PASSED lib/matplotlib/tests/test_axes.py::test_specgram_magnitude_freqs[png]
PASSED lib/matplotlib/tests/test_axes.py::test_specgram_magnitude_noise[png]
PASSED lib/matplotlib/tests/test_axes.py::test_specgram_angle_freqs[png]
PASSED lib/matplotlib/tests/test_axes.py::test_specgram_noise_angle[png]
PASSED lib/matplotlib/tests/test_axes.py::test_specgram_freqs_phase[png]
PASSED lib/matplotlib/tests/test_axes.py::test_specgram_noise_phase[png]
PASSED lib/matplotlib/tests/test_axes.py::test_psd_freqs[png]
PASSED lib/matplotlib/tests/test_axes.py::test_psd_noise[png]
PASSED lib/matplotlib/tests/test_axes.py::test_csd_freqs[png]
PASSED lib/matplotlib/tests/test_axes.py::test_csd_noise[png]
PASSED lib/matplotlib/tests/test_axes.py::test_magnitude_spectrum_freqs[png]
PASSED lib/matplotlib/tests/test_axes.py::test_magnitude_spectrum_noise[png]
PASSED lib/matplotlib/tests/test_axes.py::test_angle_spectrum_freqs[png]
PASSED lib/matplotlib/tests/test_axes.py::test_angle_spectrum_noise[png]
PASSED lib/matplotlib/tests/test_axes.py::test_phase_spectrum_freqs[png]
PASSED lib/matplotlib/tests/test_axes.py::test_phase_spectrum_noise[png]
PASSED lib/matplotlib/tests/test_axes.py::test_twin_spines[png]
PASSED lib/matplotlib/tests/test_axes.py::test_twin_spines_on_top[png]
PASSED lib/matplotlib/tests/test_axes.py::test_rcparam_grid_minor
PASSED lib/matplotlib/tests/test_axes.py::test_vline_limit
PASSED lib/matplotlib/tests/test_axes.py::test_empty_shared_subplots
PASSED lib/matplotlib/tests/test_axes.py::test_shared_with_aspect_1
PASSED lib/matplotlib/tests/test_axes.py::test_shared_with_aspect_2
PASSED lib/matplotlib/tests/test_axes.py::test_shared_with_aspect_3
PASSED lib/matplotlib/tests/test_axes.py::test_twin_with_aspect[x]
PASSED lib/matplotlib/tests/test_axes.py::test_twin_with_aspect[y]
PASSED lib/matplotlib/tests/test_axes.py::test_relim_visible_only
PASSED lib/matplotlib/tests/test_axes.py::test_text_labelsize
PASSED lib/matplotlib/tests/test_axes.py::test_pie_textprops
PASSED lib/matplotlib/tests/test_axes.py::test_tick_label_update
PASSED lib/matplotlib/tests/test_axes.py::test_margins
PASSED lib/matplotlib/tests/test_axes.py::test_length_one_hist
PASSED lib/matplotlib/tests/test_axes.py::test_pathological_hexbin
PASSED lib/matplotlib/tests/test_axes.py::test_color_None
PASSED lib/matplotlib/tests/test_axes.py::test_color_alias
PASSED lib/matplotlib/tests/test_axes.py::test_numerical_hist_label
PASSED lib/matplotlib/tests/test_axes.py::test_unicode_hist_label
PASSED lib/matplotlib/tests/test_axes.py::test_move_offsetlabel
PASSED lib/matplotlib/tests/test_axes.py::test_rc_tick
PASSED lib/matplotlib/tests/test_axes.py::test_rc_major_minor_tick
PASSED lib/matplotlib/tests/test_axes.py::test_square_plot
PASSED lib/matplotlib/tests/test_axes.py::test_no_None
PASSED lib/matplotlib/tests/test_axes.py::test_pcolorfast_colormapped[xy0-AxesImage]
PASSED lib/matplotlib/tests/test_axes.py::test_pcolorfast_colormapped[xy1-AxesImage]
PASSED lib/matplotlib/tests/test_axes.py::test_pcolorfast_colormapped[xy2-AxesImage]
PASSED lib/matplotlib/tests/test_axes.py::test_pcolorfast_colormapped[xy3-PcolorImage]
PASSED lib/matplotlib/tests/test_axes.py::test_pcolorfast_colormapped[xy4-QuadMesh]
PASSED lib/matplotlib/tests/test_axes.py::test_pcolor_fast_RGB
PASSED lib/matplotlib/tests/test_axes.py::test_shared_scale
PASSED lib/matplotlib/tests/test_axes.py::test_violin_point_mass
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs0]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs1]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs2]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs3]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs4]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs5]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs6]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs7]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs8]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs9]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs10]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs11]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs12]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs13]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs14]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs15]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs16]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs17]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs18]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs19]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs20]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs21]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs22]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs23]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs24]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs25]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs26]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs27]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs28]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs29]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs30]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs31]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs32]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs33]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs34]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs35]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs36]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs37]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs38]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs39]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs40]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs41]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs42]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs43]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs44]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs45]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs46]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs47]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs48]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs49]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs50]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs51]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs52]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs53]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs54]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs55]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs56]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs57]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs58]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs59]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs60]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs61]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs62]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs63]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs64]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs65]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs66]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs67]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs68]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs69]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs70]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs71]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs72]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs73]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs74]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs75]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs76]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs77]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs78]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs79]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs80]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs81]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs82]
PASSED lib/matplotlib/tests/test_axes.py::test_errorbar_inputs_shotgun[kwargs83]
PASSED lib/matplotlib/tests/test_axes.py::test_dash_offset[png]
PASSED lib/matplotlib/tests/test_axes.py::test_title_pad
PASSED lib/matplotlib/tests/test_axes.py::test_title_location_roundtrip
PASSED lib/matplotlib/tests/test_axes.py::test_loglog[png]
PASSED lib/matplotlib/tests/test_axes.py::test_loglog_nonpos[png]
PASSED lib/matplotlib/tests/test_axes.py::test_axes_margins
PASSED lib/matplotlib/tests/test_axes.py::test_remove_shared_axes[gca-x]
PASSED lib/matplotlib/tests/test_axes.py::test_remove_shared_axes[gca-y]
PASSED lib/matplotlib/tests/test_axes.py::test_remove_shared_axes[subplots-x]
PASSED lib/matplotlib/tests/test_axes.py::test_remove_shared_axes[subplots-y]
PASSED lib/matplotlib/tests/test_axes.py::test_remove_shared_axes[subplots_shared-x]
PASSED lib/matplotlib/tests/test_axes.py::test_remove_shared_axes[subplots_shared-y]
PASSED lib/matplotlib/tests/test_axes.py::test_remove_shared_axes[add_axes-x]
PASSED lib/matplotlib/tests/test_axes.py::test_remove_shared_axes[add_axes-y]
PASSED lib/matplotlib/tests/test_axes.py::test_remove_shared_axes_relim
PASSED lib/matplotlib/tests/test_axes.py::test_shared_axes_autoscale
PASSED lib/matplotlib/tests/test_axes.py::test_adjust_numtick_aspect
PASSED lib/matplotlib/tests/test_axes.py::test_broken_barh_empty
PASSED lib/matplotlib/tests/test_axes.py::test_broken_barh_timedelta
PASSED lib/matplotlib/tests/test_axes.py::test_axis_set_tick_params_labelsize_labelcolor
PASSED lib/matplotlib/tests/test_axes.py::test_axes_tick_params_gridlines
PASSED lib/matplotlib/tests/test_axes.py::test_axes_tick_params_ylabelside
PASSED lib/matplotlib/tests/test_axes.py::test_axes_tick_params_xlabelside
PASSED lib/matplotlib/tests/test_axes.py::test_none_kwargs
PASSED lib/matplotlib/tests/test_axes.py::test_ls_ds_conflict
PASSED lib/matplotlib/tests/test_axes.py::test_bar_uint8
PASSED lib/matplotlib/tests/test_axes.py::test_axisbelow[png]
PASSED lib/matplotlib/tests/test_axes.py::test_titlesetpos
PASSED lib/matplotlib/tests/test_axes.py::test_title_xticks_top
PASSED lib/matplotlib/tests/test_axes.py::test_title_xticks_top_both
PASSED lib/matplotlib/tests/test_axes.py::test_offset_label_color
PASSED lib/matplotlib/tests/test_axes.py::test_large_offset
PASSED lib/matplotlib/tests/test_axes.py::test_barb_units
PASSED lib/matplotlib/tests/test_axes.py::test_quiver_units
PASSED lib/matplotlib/tests/test_axes.py::test_bar_color_cycle
PASSED lib/matplotlib/tests/test_axes.py::test_tick_param_label_rotation
PASSED lib/matplotlib/tests/test_axes.py::test_fillbetween_cycle
PASSED lib/matplotlib/tests/test_axes.py::test_log_margins
PASSED lib/matplotlib/tests/test_axes.py::test_color_length_mismatch
PASSED lib/matplotlib/tests/test_axes.py::test_eventplot_legend
PASSED lib/matplotlib/tests/test_axes.py::test_bar_broadcast_args
PASSED lib/matplotlib/tests/test_axes.py::test_invalid_axis_limits
PASSED lib/matplotlib/tests/test_axes.py::test_minorticks_on[symlog-symlog]
PASSED lib/matplotlib/tests/test_axes.py::test_minorticks_on[symlog-log]
PASSED lib/matplotlib/tests/test_axes.py::test_minorticks_on[log-symlog]
PASSED lib/matplotlib/tests/test_axes.py::test_minorticks_on[log-log]
PASSED lib/matplotlib/tests/test_axes.py::test_twinx_knows_limits
PASSED lib/matplotlib/tests/test_axes.py::test_zero_linewidth
PASSED lib/matplotlib/tests/test_axes.py::test_polar_gridlines
PASSED lib/matplotlib/tests/test_axes.py::test_empty_errorbar_legend
PASSED lib/matplotlib/tests/test_axes.py::test_plot_columns_cycle_deprecation
PASSED lib/matplotlib/tests/test_axes.py::test_markerfacecolor_none_alpha[png]
PASSED lib/matplotlib/tests/test_axes.py::test_tick_padding_tightbbox
PASSED lib/matplotlib/tests/test_axes.py::test_zoom_inset
PASSED lib/matplotlib/tests/test_axes.py::test_set_position
PASSED lib/matplotlib/tests/test_axes.py::test_spines_properbbox_after_zoom
PASSED lib/matplotlib/tests/test_axes.py::test_cartopy_backcompat
PASSED lib/matplotlib/tests/test_axes.py::test_gettightbbox_ignoreNaN
PASSED lib/matplotlib/tests/test_axes.py::test_scatter_empty_data
PASSED lib/matplotlib/tests/test_axes.py::test_annotate_across_transforms[png]
PASSED lib/matplotlib/tests/test_axes.py::test_deprecated_uppercase_colors
PASSED lib/matplotlib/tests/test_axes.py::test_secondary_fail
PASSED lib/matplotlib/tests/test_axes.py::test_secondary_resize
PASSED lib/matplotlib/tests/test_axes.py::test_nodecorator
PASSED lib/matplotlib/tests/test_axes.py::test_displaced_spine
PASSED lib/matplotlib/tests/test_axes.py::test_tickdirs
PASSED lib/matplotlib/tests/test_axes.py::test_minor_accountedfor
PASSED lib/matplotlib/tests/test_axes.py::test_axis_bool_arguments[png]
PASSED lib/matplotlib/tests/test_axes.py::test_datetime_masked
PASSED lib/matplotlib/tests/test_axes.py::test_hist_auto_bins
PASSED lib/matplotlib/tests/test_axes.py::test_hist_nan_data
PASSED lib/matplotlib/tests/test_axes.py::test_hist_range_and_density
SKIPPED [1] lib/matplotlib/tests/test_axes.py:88: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:138: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:280: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:337: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:397: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:433: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:539: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:577: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:637: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:651: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:669: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:701: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:713: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:725: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:755: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:767: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:775: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:827: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:847: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:943: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:954: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:975: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1001: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1050: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1075: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1139: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1237: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1279: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1318: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1334: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1350: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1376: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1404: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1432: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1457: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1560: could not import 'pandas': No module named 'pandas'
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1569: could not import 'pandas': No module named 'pandas'
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1589: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1615: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1731: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1743: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1767: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1785: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:1810: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:2094: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:2104: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:2126: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:2392: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:2474: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:2833: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:2920: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:2989: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:3006: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:3039: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:3112: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:3122: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:3132: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:3170: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:3210: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:3220: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:3237: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:3289: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:3320: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:3497: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:3612: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:3643: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:5277: Cannot compare svg files on this system
SKIPPED [1] lib/matplotlib/tests/test_axes.py:5493: could not import 'pandas': No module named 'pandas'
SKIPPED [1] lib/matplotlib/tests/test_axes.py:5502: could not import 'pandas': No module named 'pandas'
SKIPPED [1] lib/matplotlib/tests/test_axes.py:5513: could not import 'pandas': No module named 'pandas'
SKIPPED [1] lib/matplotlib/tests/test_axes.py:5521: could not import 'pandas': No module named 'pandas'
SKIPPED [1] lib/matplotlib/tests/test_axes.py:5528: could not import 'pandas': No module named 'pandas'
SKIPPED [1] lib/matplotlib/tests/test_axes.py:6040: could not import 'pandas': No module named 'pandas'
FAILED lib/matplotlib/tests/test_axes.py::test_acorr[png] - matplotlib.testin...
FAILED lib/matplotlib/tests/test_axes.py::test_spy[png] - matplotlib.testing....
FAILED lib/matplotlib/tests/test_axes.py::test_matshow[png] - matplotlib.test...
FAILED lib/matplotlib/tests/test_axes.py::test_formatter_ticker[png] - matplo...
FAILED lib/matplotlib/tests/test_axes.py::test_formatter_ticker[pdf] - matplo...
FAILED lib/matplotlib/tests/test_axes.py::test_twin_axis_locators_formatters[png]
FAILED lib/matplotlib/tests/test_axes.py::test_twin_axis_locators_formatters[pdf]
FAILED lib/matplotlib/tests/test_axes.py::test_autoscale_tiny_range[pdf] - ma...
FAILED lib/matplotlib/tests/test_axes.py::test_basic_annotate[png] - matplotl...
FAILED lib/matplotlib/tests/test_axes.py::test_basic_annotate[pdf] - matplotl...
FAILED lib/matplotlib/tests/test_axes.py::test_polar_annotations[png] - matpl...
FAILED lib/matplotlib/tests/test_axes.py::test_polar_annotations[pdf] - matpl...
FAILED lib/matplotlib/tests/test_axes.py::test_polar_coord_annotations[png]
FAILED lib/matplotlib/tests/test_axes.py::test_polar_coord_annotations[pdf]
FAILED lib/matplotlib/tests/test_axes.py::test_polar_alignment[png] - matplot...
FAILED lib/matplotlib/tests/test_axes.py::test_fill_units[png] - matplotlib.t...
FAILED lib/matplotlib/tests/test_axes.py::test_single_point[png] - matplotlib...
FAILED lib/matplotlib/tests/test_axes.py::test_single_point[pdf] - matplotlib...
FAILED lib/matplotlib/tests/test_axes.py::test_single_date[png] - matplotlib....
FAILED lib/matplotlib/tests/test_axes.py::test_shaped_data[png] - matplotlib....
FAILED lib/matplotlib/tests/test_axes.py::test_shaped_data[pdf] - matplotlib....
FAILED lib/matplotlib/tests/test_axes.py::test_const_xy[png] - matplotlib.tes...
FAILED lib/matplotlib/tests/test_axes.py::test_const_xy[pdf] - matplotlib.tes...
FAILED lib/matplotlib/tests/test_axes.py::test_polar_wrap[png] - matplotlib.t...
FAILED lib/matplotlib/tests/test_axes.py::test_polar_wrap[pdf] - matplotlib.t...
FAILED lib/matplotlib/tests/test_axes.py::test_polar_units[png] - matplotlib....
FAILED lib/matplotlib/tests/test_axes.py::test_polar_units[pdf] - matplotlib....
FAILED lib/matplotlib/tests/test_axes.py::test_polar_rmin[png] - matplotlib.t...
FAILED lib/matplotlib/tests/test_axes.py::test_polar_rmin[pdf] - matplotlib.t...
FAILED lib/matplotlib/tests/test_axes.py::test_polar_negative_rmin[png] - mat...
FAILED lib/matplotlib/tests/test_axes.py::test_polar_negative_rmin[pdf] - mat...
FAILED lib/matplotlib/tests/test_axes.py::test_polar_rorigin[png] - matplotli...
FAILED lib/matplotlib/tests/test_axes.py::test_polar_rorigin[pdf] - matplotli...
FAILED lib/matplotlib/tests/test_axes.py::test_polar_invertedylim[png] - matp...
FAILED lib/matplotlib/tests/test_axes.py::test_polar_invertedylim_rorigin[png]
FAILED lib/matplotlib/tests/test_axes.py::test_polar_theta_position[png] - ma...
FAILED lib/matplotlib/tests/test_axes.py::test_polar_theta_position[pdf] - ma...
FAILED lib/matplotlib/tests/test_axes.py::test_polar_rlabel_position[png] - m...
FAILED lib/matplotlib/tests/test_axes.py::test_polar_rlabel_position[pdf] - m...
FAILED lib/matplotlib/tests/test_axes.py::test_polar_theta_limits[png] - matp...
FAILED lib/matplotlib/tests/test_axes.py::test_polar_theta_limits[pdf] - matp...
FAILED lib/matplotlib/tests/test_axes.py::test_axvspan_epoch[png] - matplotli...
FAILED lib/matplotlib/tests/test_axes.py::test_axvspan_epoch[pdf] - matplotli...
FAILED lib/matplotlib/tests/test_axes.py::test_axhspan_epoch[png] - matplotli...
FAILED lib/matplotlib/tests/test_axes.py::test_axhspan_epoch[pdf] - matplotli...
FAILED lib/matplotlib/tests/test_axes.py::test_hexbin_log[png] - matplotlib.t...
FAILED lib/matplotlib/tests/test_axes.py::test_nonfinite_limits[png] - matplo...
FAILED lib/matplotlib/tests/test_axes.py::test_nonfinite_limits[pdf] - matplo...
FAILED lib/matplotlib/tests/test_axes.py::test_imshow[pdf] - matplotlib.testi...
FAILED lib/matplotlib/tests/test_axes.py::test_imshow_clip[png] - matplotlib....
FAILED lib/matplotlib/tests/test_axes.py::test_imshow_clip[pdf] - matplotlib....
FAILED lib/matplotlib/tests/test_axes.py::test_polycollection_joinstyle[pdf]
FAILED lib/matplotlib/tests/test_axes.py::test_fill_between_interpolate[pdf]
FAILED lib/matplotlib/tests/test_axes.py::test_fill_between_interpolate_decreasing[pdf]
FAILED lib/matplotlib/tests/test_axes.py::test_symlog[pdf] - matplotlib.testi...
FAILED lib/matplotlib/tests/test_axes.py::test_symlog2[pdf] - matplotlib.test...
FAILED lib/matplotlib/tests/test_axes.py::test_pcolormesh[pdf] - matplotlib.t...
FAILED lib/matplotlib/tests/test_axes.py::test_pcolormesh_datetime_axis[png]
FAILED lib/matplotlib/tests/test_axes.py::test_pcolor_datetime_axis[png] - ma...
FAILED lib/matplotlib/tests/test_axes.py::test_canonical[png] - matplotlib.te...
FAILED lib/matplotlib/tests/test_axes.py::test_canonical[pdf] - matplotlib.te...
FAILED lib/matplotlib/tests/test_axes.py::test_arc_ellipse[pdf] - matplotlib....
FAILED lib/matplotlib/tests/test_axes.py::test_markevery[png] - matplotlib.te...
FAILED lib/matplotlib/tests/test_axes.py::test_markevery[pdf] - matplotlib.te...
FAILED lib/matplotlib/tests/test_axes.py::test_markevery_line[png] - matplotl...
FAILED lib/matplotlib/tests/test_axes.py::test_markevery_line[pdf] - matplotl...
FAILED lib/matplotlib/tests/test_axes.py::test_markevery_linear_scales[pdf]
FAILED lib/matplotlib/tests/test_axes.py::test_markevery_linear_scales_zoomed[pdf]
FAILED lib/matplotlib/tests/test_axes.py::test_markevery_log_scales[pdf] - ma...
FAILED lib/matplotlib/tests/test_axes.py::test_markevery_polar[pdf] - matplot...
FAILED lib/matplotlib/tests/test_axes.py::test_marker_edges[pdf] - matplotlib...
FAILED lib/matplotlib/tests/test_axes.py::test_bar_tick_label_single[png] - m...
FAILED lib/matplotlib/tests/test_axes.py::test_bar_tick_label_multiple[png]
FAILED lib/matplotlib/tests/test_axes.py::test_bar_tick_label_multiple_old_alignment[png]
FAILED lib/matplotlib/tests/test_axes.py::test_barh_tick_label[png] - matplot...
FAILED lib/matplotlib/tests/test_axes.py::test_hist_log[pdf] - matplotlib.tes...
FAILED lib/matplotlib/tests/test_axes.py::test_hist_steplog[pdf] - matplotlib...
FAILED lib/matplotlib/tests/test_axes.py::test_hist_density[png] - matplotlib...
FAILED lib/matplotlib/tests/test_axes.py::test_contour_hatching[pdf] - matplo...
FAILED lib/matplotlib/tests/test_axes.py::test_contour_colorbar[png] - matplo...
FAILED lib/matplotlib/tests/test_axes.py::test_contour_colorbar[pdf] - matplo...
FAILED lib/matplotlib/tests/test_axes.py::test_hist2d[pdf] - matplotlib.testi...
FAILED lib/matplotlib/tests/test_axes.py::test_hist2d_transpose[pdf] - matplo...
FAILED lib/matplotlib/tests/test_axes.py::TestScatter::test_scatter_plot[pdf]
FAILED lib/matplotlib/tests/test_axes.py::test_as_mpl_axes_api - assert 2 == 1
FAILED lib/matplotlib/tests/test_axes.py::test_log_scales[png] - matplotlib.t...
FAILED lib/matplotlib/tests/test_axes.py::test_log_scales[pdf] - matplotlib.t...
FAILED lib/matplotlib/tests/test_axes.py::test_stackplot[png] - matplotlib.te...
FAILED lib/matplotlib/tests/test_axes.py::test_stackplot[pdf] - matplotlib.te...
FAILED lib/matplotlib/tests/test_axes.py::test_stackplot_baseline[pdf] - matp...
FAILED lib/matplotlib/tests/test_axes.py::test_bxp_baseline[png] - matplotlib...
FAILED lib/matplotlib/tests/test_axes.py::test_bxp_rangewhis[png] - matplotli...
FAILED lib/matplotlib/tests/test_axes.py::test_bxp_precentilewhis[png] - matp...
FAILED lib/matplotlib/tests/test_axes.py::test_bxp_with_xlabels[png] - matplo...
FAILED lib/matplotlib/tests/test_axes.py::test_bxp_with_ylabels[png] - matplo...
FAILED lib/matplotlib/tests/test_axes.py::test_boxplot[png] - matplotlib.test...
FAILED lib/matplotlib/tests/test_axes.py::test_boxplot[pdf] - matplotlib.test...
FAILED lib/matplotlib/tests/test_axes.py::test_boxplot_autorange_whiskers[png]
FAILED lib/matplotlib/tests/test_axes.py::test_boxplot_rc_parameters[pdf] - m...
FAILED lib/matplotlib/tests/test_axes.py::test_vert_violinplot_baseline[png]
FAILED lib/matplotlib/tests/test_axes.py::test_vert_violinplot_showmeans[png]
FAILED lib/matplotlib/tests/test_axes.py::test_vert_violinplot_showextrema[png]
FAILED lib/matplotlib/tests/test_axes.py::test_vert_violinplot_showmedians[png]
FAILED lib/matplotlib/tests/test_axes.py::test_vert_violinplot_showall[png]
FAILED lib/matplotlib/tests/test_axes.py::test_vert_violinplot_custompoints_10[png]
FAILED lib/matplotlib/tests/test_axes.py::test_vert_violinplot_custompoints_200[png]
FAILED lib/matplotlib/tests/test_axes.py::test_horiz_violinplot_baseline[png]
FAILED lib/matplotlib/tests/test_axes.py::test_horiz_violinplot_showmedians[png]
FAILED lib/matplotlib/tests/test_axes.py::test_horiz_violinplot_showmeans[png]
FAILED lib/matplotlib/tests/test_axes.py::test_horiz_violinplot_showextrema[png]
FAILED lib/matplotlib/tests/test_axes.py::test_horiz_violinplot_showall[png]
FAILED lib/matplotlib/tests/test_axes.py::test_horiz_violinplot_custompoints_10[png]
FAILED lib/matplotlib/tests/test_axes.py::test_horiz_violinplot_custompoints_200[png]
FAILED lib/matplotlib/tests/test_axes.py::test_errorbar[png] - matplotlib.tes...
FAILED lib/matplotlib/tests/test_axes.py::test_errorbar[pdf] - matplotlib.tes...
FAILED lib/matplotlib/tests/test_axes.py::test_errorbar_limits[png] - matplot...
FAILED lib/matplotlib/tests/test_axes.py::test_errorbar_limits[pdf] - matplot...
FAILED lib/matplotlib/tests/test_axes.py::test_hist_stacked_stepfilled[png]
FAILED lib/matplotlib/tests/test_axes.py::test_hist_stacked_stepfilled[pdf]
FAILED lib/matplotlib/tests/test_axes.py::test_hist_offset[png] - matplotlib....
FAILED lib/matplotlib/tests/test_axes.py::test_hist_offset[pdf] - matplotlib....
FAILED lib/matplotlib/tests/test_axes.py::test_hist_step_horiz[png] - matplot...
FAILED lib/matplotlib/tests/test_axes.py::test_hist_stacked_weighted[png] - m...
FAILED lib/matplotlib/tests/test_axes.py::test_hist_stacked_weighted[pdf] - m...
FAILED lib/matplotlib/tests/test_axes.py::test_hist_stacked_stepfilled_alpha[png]
FAILED lib/matplotlib/tests/test_axes.py::test_hist_stacked_stepfilled_alpha[pdf]
FAILED lib/matplotlib/tests/test_axes.py::test_hist_stacked_step[png] - matpl...
FAILED lib/matplotlib/tests/test_axes.py::test_hist_stacked_step[pdf] - matpl...
FAILED lib/matplotlib/tests/test_axes.py::test_hist_stacked_density[png] - ma...
FAILED lib/matplotlib/tests/test_axes.py::test_hist_stacked_density[pdf] - ma...
FAILED lib/matplotlib/tests/test_axes.py::test_hist_stacked_bar[png] - matplo...
FAILED lib/matplotlib/tests/test_axes.py::test_hist_stacked_bar[pdf] - matplo...
FAILED lib/matplotlib/tests/test_axes.py::test_transparent_markers[pdf] - mat...
FAILED lib/matplotlib/tests/test_axes.py::test_rgba_markers[pdf] - matplotlib...
FAILED lib/matplotlib/tests/test_axes.py::test_mollweide_grid[pdf] - matplotl...
FAILED lib/matplotlib/tests/test_axes.py::test_alpha[pdf] - matplotlib.testin...
FAILED lib/matplotlib/tests/test_axes.py::test_eventplot[pdf] - matplotlib.te...
FAILED lib/matplotlib/tests/test_axes.py::test_markers_fillstyle_rcparams[png]
FAILED lib/matplotlib/tests/test_axes.py::test_eb_line_zorder[png] - matplotl...
FAILED lib/matplotlib/tests/test_axes.py::test_eb_line_zorder[pdf] - matplotl...
FAILED lib/matplotlib/tests/test_axes.py::test_vlines[png] - matplotlib.testi...
FAILED lib/matplotlib/tests/test_axes.py::test_hlines[png] - matplotlib.testi...
FAILED lib/matplotlib/tests/test_axes.py::test_step_linestyle[pdf] - matplotl...
FAILED lib/matplotlib/tests/test_axes.py::test_mixed_collection[pdf] - matplo...
FAILED lib/matplotlib/tests/test_axes.py::test_pie_default[png] - matplotlib....
FAILED lib/matplotlib/tests/test_axes.py::test_pie_linewidth_0[png] - matplot...
FAILED lib/matplotlib/tests/test_axes.py::test_pie_center_radius[png] - matpl...
FAILED lib/matplotlib/tests/test_axes.py::test_pie_linewidth_2[png] - matplot...
FAILED lib/matplotlib/tests/test_axes.py::test_pie_ccw_true[png] - matplotlib...
FAILED lib/matplotlib/tests/test_axes.py::test_pie_frame_grid[png] - matplotl...
FAILED lib/matplotlib/tests/test_axes.py::test_pie_rotatelabels_true[png] - m...
FAILED lib/matplotlib/tests/test_axes.py::test_pie_nolabel_but_legend[png] - ...
FAILED lib/matplotlib/tests/test_axes.py::test_set_get_ticklabels[png] - matp...
FAILED lib/matplotlib/tests/test_axes.py::test_retain_tick_visibility[png] - ...
FAILED lib/matplotlib/tests/test_axes.py::test_o_marker_path_snap[png] - matp...
FAILED lib/matplotlib/tests/test_axes.py::test_rc_spines[png] - matplotlib.te...
FAILED lib/matplotlib/tests/test_axes.py::test_rc_grid[png] - matplotlib.test...
FAILED lib/matplotlib/tests/test_axes.py::test_dash_offset[pdf] - matplotlib....
FAILED lib/matplotlib/tests/test_axes.py::test_auto_numticks[png] - matplotli...
FAILED lib/matplotlib/tests/test_axes.py::test_auto_numticks_log[png] - matpl...
FAILED lib/matplotlib/tests/test_axes.py::test_date_timezone_x[png] - matplot...
FAILED lib/matplotlib/tests/test_axes.py::test_date_timezone_y[png] - matplot...
FAILED lib/matplotlib/tests/test_axes.py::test_date_timezone_x_and_y[png] - m...
FAILED lib/matplotlib/tests/test_axes.py::test_titletwiny[png] - matplotlib.t...
FAILED lib/matplotlib/tests/test_axes.py::test_secondary_xy[png] - matplotlib...
FAILED lib/matplotlib/tests/test_axes.py::test_normal_axes - AssertionError: 
FAILED lib/matplotlib/tests/test_axes.py::test_get_tightbbox_polar - Assertio...
==== 167 failed, 412 passed, 73 skipped, 148 warnings in 236.61s (0:03:56) =====
+ git checkout a3e2897bfaf9eaac1d6649da535c4e721c89fa69 lib/matplotlib/tests/test_axes.py
Updated 1 path from c832619d77
