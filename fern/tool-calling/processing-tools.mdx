---
title: 'Processing Tools'
subtitle: 'Customize tool behavior by modifying schemas, inputs, and outputs'
---

Composio allows you to refine how tools interact with LLMs and external APIs through **Processors**. These are custom functions you provide to modify data at key stages:
- before the LLM sees the tool's definition
- before Composio executes the tool
- after Composio executes the tool

**Why use Processors?**

- **Improve Reliability:** Remove confusing parameters or inject required values the LLM might miss.
- **Guide LLMs:** Simplify tool schemas or descriptions for better tool selection.
- **Manage Context & Cost:** Filter large API responses to send only relevant data back to the LLM, saving tokens.
- **Adapt to Workflows:** Transform tool inputs or outputs to match your application's specific needs.

<Warning title="Python SDK Only">
Tool Processors described on this page are currently only available in Composio's **Python SDK**. Support for TypeScript is planned for the future.
</Warning>

## How Processors Work

Processors are Python functions you define and pass to `get_tools` within a `processors` dictionary. The dictionary maps the processing stage (`"schema"`, `"pre"`, `"post"`) to another dictionary, which maps the specific `Action` to your processor function.

```python Python
# Conceptual structure for applying processors

def my_schema_processor(schema: dict) -> dict: ...
def my_preprocessor(inputs: dict) -> dict: ...
def my_postprocessor(result: dict) -> dict: ...

tools = toolset.get_tools(
    actions=[Action.SOME_ACTION],
    processors={
        # Applied BEFORE the LLM sees the schema
        "schema": {Action.SOME_ACTION: my_schema_processor},

        # Applied BEFORE the tool executes
        "pre": {Action.SOME_ACTION: my_preprocessor},

        # Applied AFTER the tool executes, BEFORE the result is returned
        "post": {Action.SOME_ACTION: my_postprocessor}
    }
)
```

Let's look at each type.

## Schema Processing (`schema`)

**Goal:** Modify the tool's definition (schema) *before* it's provided to the LLM.

**Example: Simplifying `GMAIL_SEND_EMAIL` Schema**

Let's hide the `recipient_email` and `attachment` parameters from the LLM, perhaps because our application handles the recipient logic separately and doesn't support attachments in this flow.

```python Python
from composio_openai import ComposioToolSet, Action

toolset = ComposioToolSet()

def simplify_gmail_send_schema(schema: dict) -> dict:
    """Removes recipient_email and attachment params from the schema."""
    params = schema.get("parameters", {}).get("properties", {})
    params.pop("recipient_email", None)
    params.pop("attachment", None)
    # We could also modify descriptions here, e.g.:
    # schema["description"] = "Sends an email using Gmail (recipient managed separately)."
    return schema

# Get tools with the modified schema
processed_tools = toolset.get_tools(
    actions=[Action.GMAIL_SEND_EMAIL],
    processors={
        "schema": {Action.GMAIL_SEND_EMAIL: simplify_gmail_send_schema}
    }
)

# Now, when 'processed_tools' are given to an LLM, it won't see
# the 'recipient_email' or 'attachment' parameters in the schema.
# print(processed_tools[0]) # To inspect the modified tool definition
```

## Preprocessing (`pre`)

**Goal:** Modify the input parameters provided by the LLM *just before* the tool executes.

Use this to inject required values hidden from the LLM (like the `recipient_email` from the previous example), add default values, clean up or format LLM-generated inputs, or perform last-minute validation.

**Example: Injecting `recipient_email` for `GMAIL_SEND_EMAIL`**

Continuing the previous example, since we hid `recipient_email` from the LLM via schema processing, we now need to inject the correct value before Composio executes the `GMAIL_SEND_EMAIL` action.

```python Python
def inject_gmail_recipient(inputs: dict) -> dict:
    """Injects a fixed recipient email into the inputs."""
    # Get the recipient from app logic, context, or hardcode it
    inputs["recipient_email"] = "fixed.recipient@example.com"
    # Ensure subject exists, providing a default if necessary
    inputs["subject"] = inputs.get("subject", "No Subject Provided")
    return inputs

# Combine schema processing and preprocessing
processed_tools = toolset.get_tools(
    actions=[Action.GMAIL_SEND_EMAIL],
    processors={
        "schema": {Action.GMAIL_SEND_EMAIL: simplify_gmail_send_schema},
        "pre": {Action.GMAIL_SEND_EMAIL: inject_gmail_recipient}
    }
)

# Now, when the LLM calls this tool (without providing recipient_email),
# the 'inject_gmail_recipient' function will run automatically
# before Composio executes the action, adding the correct email.
# result = toolset.handle_tool_calls(llm_response_using_processed_tools)
```

<Tip title="Schema vs. Preprocessing">
Think of `schema` processing as changing the **tool's instructions** for the LLM, while `pre` processing adjusts the **actual inputs** right before execution based on those instructions (or other logic).
</Tip>

## Postprocessing (`post`)

**Goal:** Modify the result returned by the tool's execution *before* it is passed back.

This is invaluable for filtering large or complex API responses to extract only the necessary information, reducing the number of tokens sent back to the LLM, improving clarity, and potentially lowering costs.

**Example: Filtering `GMAIL_FETCH_EMAILS` Response**

The `GMAIL_FETCH_EMAILS` action can return a lot of data per email. Let's filter the response to include only the `sender` and `subject`, significantly reducing the payload sent back to the LLM.

```python Python
import json # For pretty printing example output

def filter_email_results(result: dict) -> dict:
    """Filters email list to only include sender and subject."""
    # Pass through errors or unsuccessful executions unchanged
    if not result.get("successful") or "data" not in result:
        return result

    original_messages = result["data"].get("messages", [])
    if not isinstance(original_messages, list):
        return result # Return if data format is unexpected

    filtered_messages = []
    for email in original_messages:
        filtered_messages.append({
            "sender": email.get("sender"),
            "subject": email.get("subject"),
        })

    # Construct the new result dictionary
    processed_result = {
        "successful": True,
        # Use a clear key for the filtered data
        "data": {"summary": filtered_messages},
        "error": None
    }
    return processed_result

# Get tools with the postprocessor
processed_tools = toolset.get_tools(
    actions=[Action.GMAIL_FETCH_EMAILS],
    processors={
        "post": {Action.GMAIL_FETCH_EMAILS: filter_email_results}
    }
)

# --- Simulate Execution and Postprocessing ---
# Assume 'raw_execution_result' is the large dictionary returned by
# executing GMAIL_FETCH_EMAILS without postprocessing.
# raw_execution_result = toolset.execute_action(Action.GMAIL_FETCH_EMAILS, params={...})

# Apply the postprocessor manually to see the effect (handle_tool_calls does this automatically)
# filtered_result = filter_email_results(raw_execution_result)
# print("Filtered Result (much smaller for LLM):")
# print(json.dumps(filtered_result, indent=2))
```

By using postprocessing, you can make tool results much more manageable and useful for the LLM, preventing context overflow and focusing its attention on the most relevant information.

