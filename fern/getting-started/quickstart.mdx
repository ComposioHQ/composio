---
title: 'Quickstart'
subtitle: 'Get started with using Composio'
hide-nav-links: false
---
Let's use Composio to build a workflow to star a GitHub repository using AI. Composio will discover and provide the relevant tools to the LLM and handle it's execution.

1. üîë Get your Composio API key
2. üîê Configure GitHub integration
3. üõ† Discover and fetch relevant tools
4. üß† Pass tools to an LLM
5. ‚≠ê Execute tools to star a repository



<Warning>Before proceeding, ensure you have [installed Composio](/getting-started/installation)!</Warning>

## Getting your API key
Before you begin, you'll need a Composio account. Sign up [here](https://app.composio.dev) if you haven't yet! 

Once done, you can generate the API key through either our dashboard or CLI.

<Tabs>
<Tab title="CLI">
<Steps>
<Step title="Login">
<Tabs>
<Tab title="Python">

<Warning>Ensure you have `uv` installed!</Warning>

```bash
uvx --from composio-core composio login
```

To view the API key;

```bash
uvx --from composio-core composio whoami
```
</Tab>
<Tab title="Node">
```bash
npx composio-core login
```
To view the API key;

```bash
npx composio-core whoami
```
</Tab>
</Tabs>
</Step>
<Step title="Store the API Key">
When building, store the API key in an `.env` file.
```bash
echo "COMPOSIO_API_KEY=YOUR_API_KEY" >> .env
```

Or export it to your environment variables.

```bash
export COMPOSIO_API_KEY=YOUR_API_KEY
```

</Step>
</Steps>
</Tab>
<Tab title="Dashboard">
<Steps>
<Step title="Navigate to the Dasboard">
Navigate to [Settings > Project Settings](https://app.composio.dev/developers) and create an API key from the "Project API Keys" section.
</Step>
<Step title="Store the API Key">
When building, store the API key in an `.env` file.
```bash
echo "COMPOSIO_API_KEY=YOUR_API_KEY" >> .env
```

Or export it to your environment variables.

```bash
export COMPOSIO_API_KEY=YOUR_API_KEY
```

</Step>
</Steps>
</Tab>
</Tabs>
<Note>
Make sure to not leak your Composio API key! Anyone with access to your API key can access your authenticated applications.
</Note>

## Setting Up the GitHub Integration
Before writing any code, you'll need to connect your GitHub account. Choose your
preferred method:

<Tabs>
<Tab title="CLI">
Add GitHub integration through the CLI.

<CodeGroup>
```uvx uvx
uvx --from composio-core composio add github
```
```npx npx
npx composio-core add github
```
</CodeGroup>
Follow the instructions in the CLI to authenticate and connect your GitHub account.
</Tab>
Follow the instructions in the CLI to authenticate and connect your GitHub
account.

<Tab title="Dashboard">
Head over to the [GitHub App](https://app.composio.dev/app/github).

1. Select the option to "Setup GitHub Integration"
2. Click on "Create Integration"

</Tab>
</Tabs>

## Building the Application
Now that GitHub is connected, let's make our LLM workflow:
<Steps>

<Step title="Initialize Clients">
<CodeGroup>
```python Python
from openai import OpenAI
from composio_openai import ComposioToolSet
from dotenv import load_dotenv

load_dotenv()

client = OpenAI()
toolset = ComposioToolSet()
```
```javascript JavaScript
import { OpenAI } from "openai";
import { OpenAIToolSet } from "composio-core";

const client = new OpenAI();
const toolset = new OpenAIToolSet({
    apiKey: process.env.COMPOSIO_API_KEY,
});
```
</CodeGroup>
</Step>

<Step title="Discover and Fetch Actions">

<CodeGroup>
```python Python
# Find relevant actions for our task
actions = toolset.find_actions_by_use_case(
    use_case="star a repo, print octocat",
    advanced=True,
)

# Get the tools for these actions
tools = toolset.get_tools(actions=actions)
```
```javascript JavaScript
// Find relevant actions for our task
const actionsEnums = await toolset.client.actions.findActionEnumsByUseCase({
    apps: ["github"],
    useCase: "star a repo, print octocat"
});

// Get the tools for these actions
const tools = await toolset.getTools({ actions: actionsEnums });
```
</CodeGroup>
</Step>

<Step title="Implement Tool Calling">

Let's break down the tool calling process into smaller steps to understand how
it works:

1. First, we define our task and set up the conversation with the LLM
2. Then, we enter a loop that handles the back-and-forth between the LLM and
   tools
3. Finally, we process and store the results of each tool call and break the
   loop when the task is complete.

<Steps>
<Step title="Define the Task">
<CodeGroup>
```python Python
# Define what we want the LLM to do
task = "star composiohq/composio and print me an octocat."

# Set up the initial conversation
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": task},
]
```
```javascript JavaScript
// Define what we want the LLM to do
const task = "star composiohq/composio and print me an octocat.";

// Set up the initial conversation
const messages = [
    { role: "system", content: "You are a helpful assistant." },
    { role: "user", content: task },
];
```
</CodeGroup>

</Step>

<Step title="Send Request to LLM">
<CodeGroup>
```python Python
response = client.chat.completions.create(
    model="gpt-4o",
    tools=tools,    # The tools we prepared earlier
    messages=messages,
)
```
```javascript JavaScript
const response = await client.chat.completions.create({
    model: "gpt-4o",
    tools: tools,    // The tools we prepared earlier
    messages: messages,
});
```
</CodeGroup>

<Note>
The LLM will examine our task and the available tools, then decide which tools to call and in what order.
</Note>
</Step>

<Step title="Handle Tools">
<CodeGroup>
```python Python
# Check if the LLM wants to use any tools
if response.choices[0].finish_reason != "tool_calls":
    # If no tools needed, just print the response
    print(response.choices[0].message.content)
    break

# Execute the tool calls
result = toolset.handle_tool_calls(response)

# Store the conversation history:
# 1. Store the LLM's tool call request
messages.append({
    "role": "assistant",
    "content": "",  # Empty content since we're using tools
    "tool_calls": response.choices[0].message.tool_calls,
})

# 2. Store the tool's response
for tool_call in response.choices[0].message.tool_calls:
    messages.append({
        "role": "tool",
        "content": str(result),
        "tool_call_id": tool_call.id,
    })
```
```javascript JavaScript
// Check if the LLM wants to use any tools
if (!response.choices[0].message.tool_calls) {
    // If no tools needed, just print the response
    console.log(response.choices[0].message.content);
    break;
}

// Execute the tool calls
const result = await toolset.handleToolCall(response);

// Store the conversation history:

// 1. Store the LLM's tool call request
messages.push({
    role: "assistant",
    content: "",  // Empty content since we're using tools
    tool_calls: response.choices[0].message.tool_calls,
});

// 2. Store the tool's response
messages.push({
    role: "tool",
    content: String(result),
    tool_call_id: response.choices[0].message.tool_calls[0].id,
});
```
</CodeGroup>

<Note>
This process involves three key steps:
1. Check if the LLM wants to use tools.
2. Execute the requested tool calls.
3. Store both the request and result in the conversation history.
</Note>
</Step>

<Step title="Create a loop">
Here's how all these pieces work together in a continuous loop:

<CodeGroup>
```python Python maxLines=40
# Main interaction loop
while True:
    try:
        # 1. Send request to LLM
        response = client.chat.completions.create(
            model="gpt-4o",
            tools=tools,
            messages=messages,
        )

        # 2. Check if LLM wants to use tools
        if response.choices[0].finish_reason != "tool_calls":
            print(response.choices[0].message.content)
            break

        # 3. Execute tool calls
        result = toolset.handle_tool_calls(response)

        # 4. Store the conversation history
        messages.append({
            "role": "assistant",
            "content": "",
            "tool_calls": response.choices[0].message.tool_calls,
        })
        for tool_call in response.choices[0].message.tool_calls:
            messages.append({
                "role": "tool",
                "content": str(result),
                "tool_call_id": tool_call.id,
            })
    except Exception as error:
        print(f"Error: {error}")
        if hasattr(error, 'response'):
            print(f"Response data: {error.response}")
        break
```

```javascript JavaScript maxLines=40
while (true) {
    console.log("\n‚è≥ Waiting for AI response...");
    const response = await client.chat.completions.create({
        model: "gpt-4o",
        tools: tools,
        messages: messages,
    });

    if (!response.choices[0].message.tool_calls) {
        console.log("üí¨ AI Response:", response.choices[0].message.content);
        break;
    }

    console.log("üîß Executing tool calls...");
    const result = await toolset.handleToolCall(response);
    console.log("‚úÖ Tool execution result:", result);

    messages.push({
        role: "assistant",
        content: "",
        tool_calls: response.choices[0].message.tool_calls,
    });

    messages.push({
        role: "tool",
        content: String(result),
        tool_call_id: response.choices[0].message.tool_calls[0].id,
    });
}

```
</CodeGroup>

<Note>
The loop continues until either:
- The LLM completes the task and has no more tool calls to make
- An error occurs (which would be caught by our error handling)
</Note>
</Step>

</Steps>
</Step>
</Steps>

## Full Code

Here's the full code for the workflow.

<CodeGroup>
```python Python maxLines=80
from dotenv import load_dotenv
from openai import OpenAI
from composio_openai import ComposioToolSet, App

load_dotenv()

client = OpenAI()
toolset = ComposioToolSet()

actions = toolset.find_actions_by_use_case(
    use_case="star a repo, print octocat",
    advanced=True,
)

tools = toolset.get_tools(actions=actions)
task = "star composiohq/composio and print me an octocat."
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": task},
]

while True:
    response = client.chat.completions.create(
        model="gpt-4o",
        tools=tools,
        messages=messages,
    )

    result = toolset.handle_tool_calls(response)

    if response.choices[0].finish_reason != "tool_calls":
        print(response.choices[0].message.content)
        break

    messages.append(
        {
            "role": "assistant",
            "content": "",
            "tool_calls": response.choices[0].message.tool_calls,
        }
    )

    for tool_call in response.choices[0].message.tool_calls:
        messages.append(
            {
                "role": "tool",
                "content": str(result),
                "tool_call_id": tool_call.id,
            }
        )
```
```javascript JavaScript maxLines=80
import { OpenAI } from "openai";
import { OpenAIToolSet } from "composio-core";
import dotenv from "dotenv";

// Load environment variables
dotenv.config();

async function main() {
    try {
        console.log("üöÄ Starting Composio quickstart demo...");

        // Initialize OpenAI and Composio clients
        console.log("üì¶ Initializing OpenAI and Composio clients...");
        const client = new OpenAI();
        const toolset = new OpenAIToolSet({
            apiKey: process.env.COMPOSIO_API_KEY,
        });

        console.log("üîç Finding relevant GitHub actions for the use case...");
        const actionsEnums = await toolset.client.actions.findActionEnumsByUseCase({
            apps: ["github"],
            useCase: "star a repo, print octocat"
        });
        console.log("‚úÖ Found relevant actions:", actionsEnums);

        // Get the tools for GitHub actions
        console.log("üõ†Ô∏è  Getting tools for the actions...");
        const tools = await toolset.getTools({ actions: actionsEnums });

        const task = "star composiohq/composio and print me an octocat.";
        console.log("\nüìù Task:", task);

        const messages = [
            { role: "system", content: "You are a helpful assistant." },
            { role: "user", content: task },
        ];

        console.log("\nü§ñ Starting conversation loop with AI...");
        while (true) {
            console.log("\n‚è≥ Waiting for AI response...");
            const response = await client.chat.completions.create({
                model: "gpt-4",
                tools: tools,
                messages: messages,
            });

            if (!response.choices[0].message.tool_calls) {
                console.log("üí¨ AI Response:", response.choices[0].message.content);
                break;
            }

            console.log("üîß Executing tool calls...");
            const result = await toolset.handleToolCall(response);
            console.log("‚úÖ Tool execution result:", result);

            messages.push({
                role: "assistant",
                content: "",
                tool_calls: response.choices[0].message.tool_calls,
            });

            messages.push({
                role: "tool",
                content: String(result),
                tool_call_id: response.choices[0].message.tool_calls[0].id,
            });
        }

        console.log("\n‚ú® Demo completed successfully!");
    } catch (error) {
        console.error("‚ùå Error occurred:", error);
        if (error.response) {
            console.error("üìÑ Response data:", error.response.data);
        }
    }
}

main();

```
</CodeGroup>

{/* ## Next Steps

<CardGroup>
  <Card
      title="Explore Tool Calling in Depth"
      icon="code"
      href="/concepts/tool-calling/overview"
  >
  Browse our toolset of 250+ LLM ready apps to integrate with.
  </Card>

  <Card
      title="Explore More Examples"
      icon="book-open"
      href="https://github.com/ComposioHQ/composio/tree/master/cookbook"
  >
  Check out our examples for more inspiration
  </Card>
</CardGroup> */}

<Note>
  Need help? Join our [Discord community](https://dub.composio.dev/discord)!
</Note>

